# Decision Tree
![](https://velog.velcdn.com/images/kvvon/post/20e1e6e5-851c-46f4-8e68-0a7ec9f83167/image.png)
> 위 사진과 같이 **Root부터 Leaf까지 Tree 구조를 따라가며 질문에 답해 데이터를 구분하는 모델**

- 그림처럼 **가지치기하는 것을 Puring **이라고 한다.
## 기본 원리
>**목표: 각 분류에 같은 클래스 레이블의 데이터만이 존재하도록 한다. 
Entropy (무질서도, 복잡성)이 낮아지도록 한다.** 

![](https://velog.velcdn.com/images/kvvon/post/8241372a-f4c6-4b45-98fd-84a7abfe9b4d/image.png) 이 과정을 거쳐가며 Entropy를 낮추는 것이 목표이다.

### Entropy
![](https://velog.velcdn.com/images/kvvon/post/429966bf-4509-4b78-8618-4d50ea41d2fd/image.png)

### Informaion Gain (IG)
![](https://velog.velcdn.com/images/kvvon/post/2d47cecf-a19d-47a7-bc38-123849b2d41f/image.png)
- **IG가 커질수록 분류의 성능이 좋다고 할 수 있다.**
- $k$: Subset 개수, $n_i$: $i$번째 subset 내 데이터 개수, $Entropy(i)$: $i$번째 subset의 Entropy
![](https://velog.velcdn.com/images/kvvon/post/98b87645-6e13-440e-b13f-ecd8194de493/image.png)
- 위 공식은 **Binary tree에서의 Information**이다.


## Impurity (불순도)
> **분류가 얼마나 잘 되었는지를 나타내는 척도로, Decision tree에서는 Impurity를 낮추는 것을 목표로 한다.**

우리는 **Impurity 중 Entropy, Gini impurity, Classification error** 세 가지를 사용할 것이다. ![](https://velog.velcdn.com/images/kvvon/post/ae86cec6-3c7d-42e6-b597-832ca969a12f/image.png)
- $p(i|t)$: 노드 t 속 데이터 하나를 임의로 선택했을 때, class label = $i$ 일 확률
- Entropy와 Gini impurity 의 식을 보면, **$p(i|t)=1$ 이라면 Impurity = 0 **이 되는 것을 확인 할 수 있다.

![](https://velog.velcdn.com/images/kvvon/post/6956968e-3c40-49b2-ab40-6fc5b3ec57d1/image.png)
- 세 가지의 평가 지표 모두 $p(i|t)=0.5$ 즉, 클래스 레이블이 2개인 경우에서 **Impurity가 가장 높은 경우에 각 평가지표도 최대인 것을 확인할 수 있다.  **

**Gini impurity**
- 두 클래스를 랜덤하게 선택했을 때, 다른 클래스를 선택할 확률
- $p(1-p)$

# Decision tree 구성하기
> **Impurity**를 평가지표로 사용하여 Dicision tree를 구성할 수 있다.

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree # 트리 시각화

# criterion: 분류 기준
# max_depth: leaf node 기준 parent node의 개수
tree_model = DecisionTreeClassifier(criterion= 'gini', max_depth=4, random_state=1) 
tree_model.fit(X_train, y_train)

# vstack > X_train과 X_test를 세로로 합침 (Matrix의 덧셈)
X_combined = np.vstack((X_train, X_test))
y_combined = np.vstack((y_train, y_test))

# 2차원 좌표평면으로 시각화
plot_decision_regions(X_combined, y_combined, classifier=tree_model, test_idx=range(105, 150))

feature_names = ['A', 'B', 'C', 'D']
# tree 형식으로 시각화
# feature_names=: 트리에서 사용된 특성들의 이름을 지정합니다.
#filled=True: 트리 노드를 색으로 채웁니다. 
tree.plot_tree(tree_model, features_names=features_names, filled=True)
```
- "키가 180 이상인가요?"라는 질문은 DecisionTreeClassifier가 자동으로 찾아낸 분할 기준이다다. **트리는 가능한 값들을 테스트하면서, 분할 후 데이터의 불순도 (사용자가 선택)가 최소화되는 분할 기준**을 자동으로 결정한다.
- 자동으로 찾아낸 **분할 기준에 대해서 Left node = True, Right node = False** 이다.

> 사이킷런의 Decision tree는 Parameter의 제약 내에서, 각 Class label의 impurity를 최대한 줄이는 방식으로 동작한다. 

### 학습 조절 파라미터
**max_depth**
- **기본값은 None으로 모든 노드의 Impurity = 0 이 될 때까지** 학습을 진행한다.
- **auto / sqrt**: 데이터의 특성 개수의 제곱근을 사용한다.
- **log2**: 데이터의 특성 개수의 이진 로그값을 사용한다.

**max_leaf_nodes**: Leaf node의 최대 개수를 지정, 기본값은 제한이 없다.

**min_samples_nodes**: Leaf node가 되기 위한 최소 샘플 수, 기본값은 1. 
- Overfitting을 방지한다.

**min_samples_split**: 분할을 하기 위한 최소 샘플 수, 기본값은 2. 
- Overfitting을 방지한다.

**min_impurity_decrease**: 분할로 감소되는 최소 불순도 지정. 기본값은 0이며 너무 과한 분할이 일어나지 않도록 한다.

**min_weight_fraction_leaf**: 각 리프 노드에 있어야 하는 샘플의 가중치 합의 비율. 리프 노드의 샘플 가중치의 비율이 이 값보다 적으면 분할되지 않도록 합니다.