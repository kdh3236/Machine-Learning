# Artificial Neuron
Class 0 과 Class 1, 두 개의 클래스가 있는 **Binary Classification** 작업

### 과정

1. 입력값 $x$와 가중치 $w$의 Linear combination으로 결정 함수 $\sigma(z)$를 정의한다.

	- $z=w_1x_1 + w_2x_2 + ... + w_mx_m$

2. $u(x) = \begin{cases} 1 & \text{if } x \geq \theta \\ 0 & \text{if } x < \theta 
\end{cases}$
	
3. 위의 식을 간편하게 조정해보자.
	
    - $z - \theta \geq 0$
    - $z = w_1x_1 + w_2x_2 + ... + w_mx_m + b = W^TX + b$ 

4. (3)의 결과는 다음과 같다.
$u(x) = \begin{cases} 1 & \text{if } z \geq 0 \\ 0 & \text{if } z < 0 
\end{cases}$

## 퍼셉트론 학습 규칙

1. 가중치를 0 또는 굉장히 작은 값으로 초기화한다.
2. 각 훈련 샘플 $x^i$에서 다음 작업을 한다.
	
    - 출력값 $\hat{y}$를 계산한다.
    - $W, b$ 를 업데이트한다.
    
### Parameter 업데이트
>$W^i = W^i + \Delta{W^i}$
$b^i = b^i + \Delta{b^i}$

>$\Delta{W^i} = \eta(y^i - \hat{y}^i)X^i$
$\Delta{b^i} = \eta(y^i - \hat{y}^i)b^i$

$W^i$: i번째 훈련 샘플에 대한 가중치 행렬
$b^i$: i번째 훈련 샘플에 대한 $b$
$X^i$: i번째 훈련 샘플에 대한 $x$ 행렬
$\eta$: Learning rate (0 ~ 1)

$y^i$는 데이터의 실제 분류 (0 또는 1), $\hat{y}^i$은 퍼셉트론을 통한 예측 분류이다.

**모델이 정확히 분류한 경우는 $\Delta = 0$ 이기 때문에 업데이트 되지 않지만, 정확히 분류하지 않은 경우에는 해당 훈련 샘플 방향으로 parameter를 조정하게 된다.**

위 퍼셉트론을 이용한 분류는 **모든 데이터가 Linear Boundary로 완벽히 분류가 가능한 경우에만 수렴이 보장**된다.

- Linear Boundary로 분류가 불가능한 경우에는 **epoch**를 설정하고 분류 허용 오차를 지정함으로써 분류할 수 있다.
- **epoch**: 훈련 데이터셋에 대해 학습을 반복할 최대 횟수

![](https://velog.velcdn.com/images/kvvon/post/62e8c867-cf19-4bd7-9cc2-e2cc65042666/image.png)


# 파이썬으로 퍼셉트론 학습 알고리즘 구현
### 학습

```python
import numpy as np

class Perceptron:
    def __init__(self, eta=0.01, n_iter=50, random_state=1):      
        self.eta = eta #학습률
        self.n_iter = n_iter #반복 횟수 (epochs)
        self.random_state = random_state (난수 설정)
    
    def fit(self, X, y):
        rgen = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc = 0.0, scale=0.01, size=X.shape[1]) #가중치를 정규 분포를 따르는 난수로 설정
        self.b_ = np.float64(0.)
        self.errors_ = [] #예측과 다른 데이터를 저장할 배열

        for _ in range(self.n_iter):
            errors = 0
            for xi, target in zip(X, y):
                update = self.eta * (target - self.predict(xi)) # 학습률 * (오차)
                self.w_ += update * xi #가중치 업데이트
                self.b_ += update #bias 업데이트
                errors += int(update != 0.0) 
            self.errors_.append(errors)
    
    def net_input(self, X):
        return np.dot(X, self.w_) + self.b_ #W^tX + B

    def predict(self, X):
        return np.where(self.net_input(X) >= 0.0, 1, 0) #classification
```

### 시각화 (분류)
```python
from matplotlib.colors import ListedColormap

def plot_decision_regions(X, y, classifier, resolution=0.02):
    makers = ('o', 's', '^', 'v', '<')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])

    x1_min, x1_max = X[:, 0].min() - 1, X[:,0].max() + 1
    x2_min, x2_max = X[:, 0].min() - 1, X[:,0].max() + 1
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), 
    	np.arange(x2_min, x2_max, resolution)) #grid 설정
        #xx1은 같은 숫자는 같은 열에, xx2는 같은 행에 저장됨 
    lab = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    #np.array([xx1.ravel(), xx2.ravel()]).T: 격자점 생성 후 Transpose
    lab = lab.reshape(xx1.shape) #배열 크기 맞춰주기 (행 기준)
    plt.contourf(xx1, xx2, lab, alpha = 0.3, cmap=cmap) 
    #xx1, xx2로 형성된 격자판에 lab 결과에 따라 색깔을 나눔
    #정수 좌표들로만 계산하지만 전체 판이 채워짐
```

**Grid**:  2D 공간을 일정한 간격으로 나누는 격자 모양의 구조
![](https://velog.velcdn.com/images/kvvon/post/6cbb1ced-de33-4a04-9cfd-658ee6d68536/image.png)![](https://velog.velcdn.com/images/kvvon/post/c0e4b7d5-39a1-41dc-a46f-181112bc421c/image.png)

**contourf()** 실행 결과
![](https://velog.velcdn.com/images/kvvon/post/d5c4e29d-c238-4b1c-9f69-a12b644711d7/image.png)

