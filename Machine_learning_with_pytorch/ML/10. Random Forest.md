# Ensemble (앙상블)
>**여러 단순한 모델을 결합하여 정확한 모델을 만드는 방법**

Random forest에서는 Bagging을 사용할 것이다. 간단히만 살펴보자. ![](https://velog.velcdn.com/images/kvvon/post/d8796f05-5ff9-4193-80d3-2abf22694ecf/image.png)


# Random forest
> **여러 개의 Dicision tree의 Ensemble**
**여러 개의 Dicision tree의 평균을 내어 분류하는 머신 러닝 기법**

![](https://velog.velcdn.com/images/kvvon/post/abcac127-61c7-4cdf-8104-fa094a19b0e9/image.png)

## 과정
1. n개의 랜덤한 Bootstrap 샘플을 뽑는다.
	
    - Training dataset에서 중복을 허용하며 랜덤하게 n개의 샘플을선택한다. 

2. Bootstrap 샘플에서 Decision tree를 학습한다.
	
    - 중복을 허용하지 않고 $d$ 개의 특성을 선택하고 각 decision tree를 학습시킨다.
    
3. (1) ~ (2) 단계를 k번 반복한다.

4. 각 Decision tree에서의 예측을 모아 **Marjority voting**으로 Class label을 할당한다.

## 장점
1. 각 **Decision tree의 Parameter를 세밀하게 지정할 필요가 없다.**
	
    - 하이퍼파라미터 튜닝에 많은 노력이 필요하지 않다.
2. 각 Decision tree의 분산이 높고, Overfitting의 가능성이 있지만 **Random forest는 일반화 성능을 높이고 Overfitting의 위험을 줄인다.**

## Parameter

1. $n$: Bootstrap sample 크기
	- **n이 작다면?** 
    	
        - 특정 훈련 샘플이 사용될 확률이 낮기 때문에 트리의 다양성이 증가한다.  
        - Random forest의 무작위성이 증가하고 Overfitting의 가능성이 줄어든다.
        - 전체적인 성능이 줄어든다.
    - **n이 크다면?**
    	
        - Overfitting의 가능성이 늘어난다.
2. $d$: 각 decision tree에서 랜덤하게 선택할 특성의 개수
	
    - 보통 훈련 데이터셋의 전체 특성 개수보다 작게 지정한다.
    
> **일반적으로 $n$ = (훈련 데이터 내의 샘플 개수), $d$ = $\sqrt{(훈련데이터셋 내 특성 개수)}$ 이다** 

# 사이킷런으로 Random forest 구현
```python
from sklearn.ensemble import RandomForestClassifier

# n_estimators=: 몇 개의 decision tree를 사용할 것인지
# n_jobs=: 해당 개수의 CPU 코어를 사용하여 병렬 처리로 학습 속도를 높인다
# impurity: 기본적으로 Gini impurity 사용
# max_features=: 기본값으로 auto, sqrt가 설정되어져 있다.
forest = RandomForestClassifier(n_estimators=25, random_state=1. n_jobs=2)
forest.fit(X_train, y_train)

plot_decision_regions(X_combined, y_combined, classifier=forest, test_idx=range(105, 150))
```