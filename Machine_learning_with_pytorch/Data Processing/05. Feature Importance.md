**Random forest와 같은 tree-based model의 장점은 표준화나 정규화를 할 필요가 없다는 점이다.**

> **Random forest는 분류에 사용한 특성별로 특성 중요도 즉, 분류에 어느정도 영향을 미쳤는지를 나타내는 지표를 저장한다.**

## 사이킷런으로 특성 중요도 확인
``` python
from sklearn.ensemble import RandomForestClassifier
feat_labels = df_wine.columns[1:]

forest = RandomForestClassifier(n_estimators=500, random_state=1)
forest.fit(X_train, y_train)

# 특성 중요도 확인
# np.argsort(importances): 작은 값부터 순서대로 정렬된 인덱스를 반환
# [:: -1]: 내림차순 정렬 (중요도가 높은 순서로 정렬)
importances = forest.feature_importances_
indices = np.argsort(importances)[::-1]
```

## 임계값을 넘지 못하는 특성 중요도를 가진 특성 제거
```python
from sklearn.feature_selection import SelectFromModel

# threshold=: 임계값 설정
# prefit=True: forest가 이미 학습된 모델임을 확인함
sfm = SelectFromModel(forest, threshold=0.1, prefit=True)
X_selected = sfm.transform(X_train)
```

## Recursive Feature Elimination
**과정**
1️⃣ 전체 특성으로 모델을 학습
2️⃣ 특성 중요도(feature importance) 를 기반으로 가장 덜 중요한 특성 제거
3️⃣ 남은 특성으로 다시 학습 후 반복
4️⃣ 지정한 개수만큼 특성이 남을 때까지 반복

```python
from sklearn.feature_selection import RFE

# n_features_to_select 개의 특성만이 남을 때까지 동작
rfe = RFE(forest, n_features_to_select=5)
rfe.fit(X_train, y_train)
```