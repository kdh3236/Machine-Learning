{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6cae136-e4ba-4dd3-aea3-e7bcb711bb16",
   "metadata": {},
   "source": [
    "12_2의 코드에서는 **Activation function으로 Sigmoid function을** 사용하였다.\n",
    "\n",
    "하지만, 기술적으로는 **미분이 가능하다면** 어떤 함수라도 **Activation function**으로 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3d2c60-e3b2-47c6-9fc2-057dd67cb6ff",
   "metadata": {},
   "source": [
    "이론적으로, **Sigmoid 함수가** 뉴런 개념을 가장 비슷하게 흉내내지만, 만약 입력 $x$가 매우 작은 음수라면, **Vanishing Gradient 문제가** 발생하여 학습이 어려워진다.\n",
    "\n",
    "**Loss = 0**이라면 **Gradient가 0에 가까워져** Weight가 업데이트되지 않는다.\n",
    "\n",
    "이 경우, **Local Minimum 등에 갇히게 되거나 학습이 진행되지 않을 수 있다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cdc1a3-dc38-4b46-97ca-39ae0098038b",
   "metadata": {},
   "source": [
    "따라서 대신 **tanh 함수를 더 선호한다.**\n",
    "\n",
    "아래에선 여러 종류에 **Activation Function**에 대해 알아볼 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9eeb51-42ad-4e65-be4a-1174e8b86e78",
   "metadata": {},
   "source": [
    "먼저 **Logistic function에** 대해 알아보자\n",
    "- **Logistic functiond은** **Sigmoid function의 특별한 경우이다.**\n",
    "\n",
    "$z = w^Tx$ 일 때, **Logistic function**은 $\\frac{1}{1 + \\exp(-z)}$ 으로 구할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfcb748a-1588-44ed-acfc-cc60c1815749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(y=1|x) = 0.888\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([1, 1.4, 2.5]) # W0 절편때문에 X의 첫 번째 원소는 반드시 1이여야 한다.\n",
    "w = np.array([0.4, 0.3, 0.5])\n",
    "\n",
    "def net_input(X, w):\n",
    "    return np.dot(X, w)\n",
    "\n",
    "def logistic(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def logistic_activation(X, w):\n",
    "    z = net_input(X, w)\n",
    "    return logistic(z)\n",
    "\n",
    "print('P(y=1|x) = %.3f' % logistic_activation(X, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cf6a1b-9a0c-4778-8505-ba7675d0c042",
   "metadata": {},
   "source": [
    "**P(y=1|x) = 0.888** 이 결과는 **x가 1번 class에 속할 확률이 88.8%라는** 의미이다.\n",
    "\n",
    "**이진 분류**에 적합한 **Logistic Regression**은 **다중 클래스 문제에서는 직접 적용하기 어렵다.** \n",
    "\n",
    "여러 개의 로지스틱 유닛을 **병렬로 사용하여 다중 클래스 출력을 구성**하면, 각 클래스의 출력이 **독립적인 확률처럼 동작**하게 되어, **전체 확률의 합이 1이 되지 않아** 해석이 어렵다. \n",
    "\n",
    "이러한 문제를 해결하기 위해 **소프트맥스(softmax) 함수를 사용하는 다중 클래스 로지스틱 회귀(softmax regression)를** 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce7fffe-8b81-4dbb-bd91-19a8a12f0a76",
   "metadata": {},
   "source": [
    "**tanh**함수의 수식은 이와 같다.\n",
    "- $\\frac{e^x - e^{-x}}{e^x + e^{-x}}$\n",
    "\n",
    "**sigmoid function**과 비교했을 때, **tanh** 함수는 아래와 같이 나타낼 수 있다.\n",
    "\n",
    "- **2 x sigmoid(2z) - 1**\n",
    "\n",
    "**sigmoid function의 결과의 범위가 (0, 1)인 것을 감안하면,** **tanh 함수의 범위는 (-1, 1) 인 것을 확인할 수 있다.**\n",
    "\n",
    "즉, **tanh** 함수는 **sigmoid** 함수보다 출력 범위를 **2배 넓혀서** 알고리즘의 수렴 범위를 향상시킬 수 있다.\n",
    "\n",
    "- **이 것은 Vanishing Gradient 문제가 해결되는 것을 의미한다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "957c1fc5-cd53-4842-91c3-d89995838e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch나 Numpy에 tanh를 지원하는 함수가 있다.\n",
    "import torch\n",
    "\n",
    "# np.tanh(X)\n",
    "\n",
    "# torch.tanh(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc83d94-4335-4bcd-bebb-831793a67d99",
   "metadata": {},
   "source": [
    "**ReLu** 함수는 max(0, z)로 나타낼 수 있다.\n",
    "\n",
    "- 즉, 음수 부분을 0으로 대체하는 함수이다.\n",
    "\n",
    "**sigmoid 함수에서** 출력값이 굉장히 작은 음수라면, **Vanishing Gradient 문제**가 발생한다고 하였는데 이를 방지하면서 **비선형성**을 부여하는 Activation function이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1d197cc-6a85-4a74-8ca8-493befc94d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.relu() 함수를 이용하여 구현할 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
