# Adaline (적응형 선형 뉴런)
![](https://velog.velcdn.com/images/kvvon/post/0361e62b-6130-4b43-a8eb-c0892e812790/image.png)
- **퍼셉트론과의 차이점**: 실제 class label과 선형 활성화 함수의 실수 출력 값을 비교하여 모델의 오차를 계산하고 가중치를 업데이트한다.

## Mean Square Error (MSE)
지도 학습 알고리즘에서 목적 함수로서 사용하는 손실 함수 L
![](https://velog.velcdn.com/images/kvvon/post/84939be6-9a95-4e14-9572-c5f2dcdecf93/image.png)

# Gradient descent (경사 하강법)
- 학습을 진행하며 손실 함수 L의 최솟값을 찾기 위한 알고리즘 중 하나
- 손실 함수의 **Gradient $\nabla L(w, b)$** 의 반대 방향으로 업데이트한다.
![](https://velog.velcdn.com/images/kvvon/post/c0ad6ec8-b244-442a-9070-53fb76bfccfb/image.png)

## Full-batch gradient descent
전체 훈련 데이터셋을 기준으로 parameter를 업데이트하는 방법
- $w: w + Δ
w = w - \frac{\partial L(w,b)}{\partial w}$
- $b: b + Δb = b - \frac{\partial L(w,b)}{\partial b}$
- 훈련 데이터셋이 큰 경우, 계산 비용이 매우 커진다는 단점이 있다.
    
**MSE를 미분한 것은 다음과 같다.** ![](https://velog.velcdn.com/images/kvvon/post/8dea53a3-5f73-4183-8b12-fa34162851db/image.png)

### 학습
```python
import numpy as np

class AdalineGD:
    def __init__(self, eta=0.01, n_iter=50, random_state=1):      
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state
    
    def fit(self, X, y): #X: matrix, y: vector
        rgen = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc = 0.0, scale=0.01, size=X.shape[1])
        self.b_ = np.float64(0.)
        self.losses_ = []

        for _ in range(self.n_iter):
            net_input = self.net_input(X)
            output = self.activation(net_input)
            errors = (y-output)
            self.w_ += self.eta * 2.0 * X.T.dot(errors) / X.shape[0]
            # 위 식은 gradient W와 동일
            #X.shape = (N,) tuple 반환
            self.b_ += self.eta * 2.0 * errors.mean() 
            #errors.mean() 은 sum(errors) / X.shape[0] 과 동일하다.
            loss = (errors**2).mean() 
            self.losses_.append(loss) #각 Epoch마다 loss 확인하기 위함
        return self
    
    def net_input(self, X):
        return np.dot(X, self.w_) + self.b_ //W^tX + b

    def activation(self, X): #여기서는 특별한 의미가 없지만 신경망의 개념을 보여주기 위함
        return X

    def predict(self, X):
        return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)

```

### $w_j$ 를 이용하여 구하는 방법
```python
for w_j in range(self.w_.shape[0]):
	self.w_[w_j] += self.eta * (2.0 * (X[:, w_j] * errors)).mean()
```

### 최적의 학습률을 선택해야하는 이유
![](https://velog.velcdn.com/images/kvvon/post/7e79ea66-921f-4e03-bc6b-2fe1d123d401/image.png) 

1. **학습률이 너무 작다면**
	
    - 손실 함수 L을 최소화하기 위해 많은 Epoch가 필요하다.

2. **학습률이 너무 크다면**
	
    - 각 Epoch마다 변화량이 커져 손실 함수 최솟값을 지나칠 수 있다.
    
## Stochastic gradient descent
모든 데이터셋에 대해 Parameter를 업데이트하지 않고 각 훈련 샘플에 대해 Parameter를 업데이트 하는 방법
- 더 빠르게 수렴할 가능성이 높다.

**Online gradient descent**라고도 한다.
- **Online learning**: 새로운 훈련 데이터가 발생할 때마다 업데이트하는 방법

### 학습
```python
import numpy as np

class AdalineSGD:
    def __init__(self, eta=0.01, n_iter=50, shuffle=True, random_state=1):      
        self.eta = eta
        self.n_iter = n_iter
        self.w_initialized = False #가중치 초기화 여부
        self.shuffle = True #random한 순서 보장
        self.random_state = random_state
    
    def fit(self, X, y): #X: matrix, y: vector   
        self._initialize_weights(X.shape[1])
        self.losses_ = []
        for i in range(self.n_iter):
            if self.shuffle:
                X, y = self._shuffle(X, y) 
            losses = []
            for xi, target in zip(X, y):
                losses.append(self._update_weights(xi, target))
            avg_loss = np.mean(losses)
            self.losses_.append(avg_loss)
        return self
        
    def partial_fit(self, X, y):
        if not self.w_initialized:
            self._initialize_weights(X.shape[1])
        if y.ravel().shape[0] > 1:
            for xi, target in zip(X, y):
                self._update_weights(xi, target)
                #샘플 단위로 업데이트되는 부분이다.
        else:
            self._update_weights(X, y)
        return self
    
    def _shuffle(self, X, y):
        r = self.rgen.permutation(len(y)) 
        #0 ~ len(y)-1 까지의 배열을 랜덤하게 섞어 배열로 반환
        return X[r], y[r] 
        #배열을 인덱스로 사용하면 r[0], r[1], ...에 맞는 위치의 element로 이루어진 배열이 반환환

    def _initialize_weights(self, m):
        self.rgen = np.random.RandomState(self.random_state)
        self.w_ = self.rgen.normal(loc = 0.0, scale=0.01, size=m) 
        #가중치 벡터를 서로 겹치지 않는 난수로 초기화
        self.b_ = np.float64(0.)
        self.w_initialized = True

    def _update_weights(self, xi, target): 
        #각 샘플마다 Parameter를 조정하기 위한 함수수
        output = self.activation(self.net_input(xi))
        error = (target - output)
        self.w_ += self.eta * 2.0 * xi * error
        self.b_ += self.eta * 2.0 * error
        loss = error ** 2
        return loss

    def net_input(self, X):
        return np.dot(X, self.w_) + self.b_

    def activation(self, X): #여기서는 특별한 의미가 없지만 신경망의 개념을 보여주기 위함
        return X

    def predict(self, X):
        return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)

	#온라인 학습 방식으로 훈련시키려면?
	AdalineSGD.partial_fit(X[0,:], y[0])
```

#### 효율적인 학습을 위해서는?
1. 훈련 데이터의 순서를 무작위로
2. 학습률을 적응형 학습률로 대체
	
    - $\frac{c_1}{[number of iterations] + c_2}$

## Mini-batch gradient descent
Full-batch gradient descent와 Stochastic gradient descent 사이의 절충점
- 훈련 데이터셋을 몇 개의 집합으로 나누어 집합마다 업데이트를 진행하는 방법
    
# Analytical solution
수학적이고 닫힌 형태의 해를 계산적으로 구하는 방법

- $\frac{\partial L(w,b)}{\partial w} = \frac{\partial L(w,b)}{\partial b} = 0$ 을 수학적으로 구함으로써 최적의 Parameter인 w, b를 찾을 수도 있다. 

