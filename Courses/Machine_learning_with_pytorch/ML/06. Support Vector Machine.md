### Hyperplane
>Hyperplane: 아래와 같이 $w^Tx=b$를 만족하는 데이터 포인트 $x$의 집합
**{ $x | w^Tx=b$ }**

![](https://velog.velcdn.com/images/kvvon/post/7bf05f3d-9b7b-495c-8e9d-23222ed56cae/image.png)
   
   
# Support vector machine

> **Margin을 최대화하여 데이터를 분류하는 학습 알고리즘**

![](https://velog.velcdn.com/images/kvvon/post/35ec71d7-8ea0-49d2-9a83-1316c1e80d37/image.png)
- **Support vector:** 각 클래스 내에서 Maximum margin hyperplane (결정 경계)로 부터 가장 가까운 vector  
	
    - Support vector라고 부르는 이유는 해당 vector에 따라 Hyperplane의 위치가 변하기 때문이다.
- **Margin:** Maximum margin hyperplane과 positive (negative) hyperplane 사이의 거리
	
    - n차원 공간에서 두 데이터를 분류할 수 있는 **(n-1) 차원의 Hyperplane**을 찾는다고 생각하면 된다.
    - 이때, **모든 Hyperplane은 가중치 벡터인 $w$ 와 직교한다.**
    
>**목표:** **가중치 벡터 $w$ 를 찾아 데이터를 분류하는 Hyperplane을 찾자**

### Margin을 최대화하는 이유
**수학적 증명**
![](https://velog.velcdn.com/images/kvvon/post/d7b6b017-a6a9-418c-82db-61843e6d90ae/image.png)
- **Lagrange Multipliers 사용 이유**
	
    - 제약 조건이 포함된 최적화 문제는 직접 풀기 어렵기 때문에, 라그랑주 승수법을 이용해 푸는 경우가 많다.
    
    - 제약 조건이 있는 최적화 문제를 풀 때, 제약 조건을 만족시키는 범위 내에서 목적 함수를 최적화해야한다.
    - **Object function에 제약 조건을 추가하기 위해 라그랑주 승수 $\alpha_𝑖$ 를 사용한다.** 

- **Dual problem 사용 이유**
	![](https://velog.velcdn.com/images/kvvon/post/524b96ad-2fd3-42eb-b3e2-613b56b0e56d/image.png)
### 제약 조건
![](https://velog.velcdn.com/images/kvvon/post/041b0e6b-ead8-4138-bf81-08b45fc2cda3/image.png) ![](https://velog.velcdn.com/images/kvvon/post/ad5e3446-5872-474f-b419-db540fb3b20e/image.png) **제약 조건 추가 방법**
![](https://velog.velcdn.com/images/kvvon/post/8819de87-c199-4e27-8301-ace78958d34a/image.png) ![](https://velog.velcdn.com/images/kvvon/post/f564e3db-3035-447c-8c8c-3ce7b23b0b9f/image.png)
- g(x) $\ge$ 인데도 빼는 이유는 우리는 L(x)를 최소화하기를 원하기 때문이다.
	
    - 생각해보면 제약 조건을 어겼을 경우, $g(x) < 0$ 인 경우에 L(x) 가 증가하는 것을 확인할 수 있다.
## Soft margin classification
> **완벽히 선형적으로 구분되지 않는 데이터를 SVM을 이용하여 구분할 수 있도록 하는 SVM**

### Slack variable
> 선형적으로 구분되지 않는 데이터에서 SVM의 최적화 목적 함수에 있는 선형 제약 조건을 완화할 필요가 있기 때문에 사용하는 변수 

![](https://velog.velcdn.com/images/kvvon/post/9de5c281-adbe-4406-a5cf-8323bb2a63db/image.png) 
- 잘못 분류된 데이터를 나타낸다.
- 직선을 그었을 때, 데이터가 속한 클래스와 내가 예측한 해당 데이터의 y값의 차이를 뜻한다.

### 목적 함수 (Primal problem)

![](https://velog.velcdn.com/images/kvvon/post/327be91c-d520-44fa-91cc-58d8355d9874/image.png) 
- C 값이 클수록 규제가 강하고, C 값이 작을수록 규제가 약하다.

>** Margin에서 먼 정도인 슬랙 변수를 최소화 식에서 더한다는 것**: Margin에서 먼 것을 최소화 한다.
**이것은 곧 Margin에서 먼 데이터가 Margin 내에 어느정도 포함된다면 결과적으로 마진이 커지는 것을 이끌어낼 수 있다는 것이다.**

![](https://velog.velcdn.com/images/kvvon/post/ad71210f-7055-4a79-ae1b-ada41cfe0330/image.png) ![](https://velog.velcdn.com/images/kvvon/post/89671f43-9f08-41bb-a3da-86132d808859/image.png)
- 위 제약 조건을 통해 SVM은 **어느 정도의 오차를 허용**하고 비선형 데이터의 분류가 가능하도록 한다. 
>- **Margin에서 크게 벗어나있던 데이터에 한하여, margin 내부로 들어오는 것을 허용하며 동시에 Hyperplane을 조절한다**

![](https://velog.velcdn.com/images/kvvon/post/192d1c18-903d-4a81-8f4f-e096f7be124f/image.png)
- **Dual form**은 아래와 같다. ![](https://velog.velcdn.com/images/kvvon/post/dbdc45b6-2222-449d-bba5-b8f70089c04d/image.png)
- 이때, Dual form에서는 **Slack variable**의 여부와 상관없이 형태가 같은 것을 알 수 있다.
	
    - 이유는 Supprot vector와 관련된 Lagrange multiplier인 $\alpha$값들을 최적화하는 데 초점을 맞춘다.
    
### 사이킷런으로 SVM with Slack variable 구현하기
```python
from sklearn.svm import SVC

svm = SVC(kernel='linear', C=1.0, random_state=1)
svm.fit(X_train_std, y_train)
```

- 사이킷런의 SVM은 **기본적으로 Slack variable이 사용된다.**
- Parameter의 **C값을 줄이면 Underfitting의 가능성이 늘고, 늘리면 Overfitting의 가능성이 는다. **

## Hard margin classification
> **완벽히 선형 분리가 가능한 데이터에 대해 사용하는 SVM을 뜻한다.**

    
### 사이킷런으로 Hard margin classification 구현하기
- 사이킷런에는 따로 Hard margin classification가 지원되지 않는다.
	
    - 사이킷런의 SVM에는 slack variable이 필수이기 때문이다.
    - **한가지 방법은 slack variable의 C값을 최소화하는 것이다.**
    
```python
from sklearn.svm import SVC

# 두 데이터 포인트 x_i, x_j의 내적을 그대로 사용하는 커널
svm = SVC(kernel='linear', C=0.00001, random_state=1) #C값을 최소화
svm.fit(X_train_std, y_train)
```

# LIBSVM 라이브러리
> **SVM을 구현하는 데 있어 매우 강력하고 효율적인 라이브러리**

- 속도가 굉장히 빠르다.
- **kernel='linear'** 코드에 linear 대신 다른 알고리즘을 사용할 수 있도록 해준다.

# 사이킷런의 다른 구현
>사이킷런에서 **데이터셋이 너무 커서 컴퓨터 메모리 용량에 맞지 않는 경우에** 지원해주는 **SGDClassifier** 클래스를 사용할 수도 있다.

```python
from sklearn.linear_model import SGDClassifier

# 퍼셉트론 사용
ppn = SGDClassifier(loss='perceptron')
# 로지스틱 회귀 사용
lr=SGDClassifier(loss='log')
# svm 사용, hinge loss는 margin을 최대화하는 방식의 loss이다.
svm=SGDClassifier(loss='hinge')
```

세 가지 모두 사용되는 머신러닝 기법만 다르며 작동 방식은 **아달린을 위해 구현한 stochastic gradient descent와 비슷하다.**
![](https://velog.velcdn.com/images/kvvon/post/290f68f5-cb26-423a-9fb5-f08ee08371ee/image.png)
