 

`Fully-connected Neural Network`는 다양한 함수들을 표현할 수 있지만, **입력 이미지의 2차원 `Spatial structure`를 고려하지 않는다는 문제점**이 있다.

![alt text](image-189.png)

- 지금까지는 2차원 이미지 배열을 **Flatten**하여 사용해서, `Spatial structure`가 제대로 보존되지 않았다.

이 문제점을 **`Spatial Structure`를 보존하는 연산자나 모델**을 새로 정의하여 해결할 수 있다.


# Convolution Network

`Fully-connected linear classifier`에서 사용하는 2개의 Element에 더하여 **아래 3개의 Element가 추가적으로 필요**하다.

![alt text](image-190.png)

`Convolution Network`를 구성하는 3개의 Element를 순서대로 살펴보자.

# 1. Convolution Layers

> **이미지 배열의 2차원 Spatial sturcture을 보존할 수 있도록 한다.**

우선, `Fully-connected layer`가 vector를 입력받았던 것과 다르게 **3차원 Tensor를 입력**으로 받는다.

입력이 3차원 Tensor이기 때문에, **`Weight` 역시 3차원 Tensor의 형태**를 가진다.
- **단, 반드시 `Input`의 `Depth`와 `Weight`의 `Depth`는 동일해야 한다.**

![alt text](image-191.png)

이후, **입력 이미지에서 `Weight = Filter`에 맞는 크기만큼의 `Chunk`에 대해 `Inner Product`를 실행하고 `Bias`를 더한다.**
- 이 결과, Scalar 값이 결과로 나온다.

- 이 계산 결과는 **입력 Tensor의 각 Element가 대응되는 `Filter`의 각 Element와 얼마나 일치**하는지에 대한 정보이다.

![alt text](image-192.png)

같은 `Filter`에 대해 입력 이미지에서 `Filter`를 놓을 수 있는 모든 가능한 공간에 대해 적용하고 대응되는 위치에 저장하면  `Activation map`이 나온다.

- `Activation map`이라고 부르는 이유는 **각 Element가 입력의 각 부분이 `Filter`에 얼마나 많은 응답**을 하는지 보여주기 때문이다.

![alt text](image-193.png)

정확히 동일한 동작을 다른 값을 가진 `Weight`에 대해 적용할 수 있다.

![alt text](image-194.png)

6개의 `Filter`의 대해 적용하는 `Convolution Layer`가 있다고 가정하면 아래와 같은 결과를 얻을 수 있다.
- 6개의 `Activation map`을 얻을 수 있으며 이를 모두 연결하면 **6 * 28 * 28 크기**의 결과를 얻을 수 있다.

![alt text](image-195.png)

또한 각 `Filter`마다 각각의 `Bias term`을 가질 수 있다.
`Bias term`까지 적용한 결과는 아래와 같다.
- 이때, 한 차원의 `Bias`는 28 * 28 grid에 **전부 같은 값이 더해지도록** 한다.

![alt text](image-196.png)

결과로 나온 `Activation map`에 대해 **두 가지 설명**을 할 수 있다.

1. 출력을 **6 * 28 * 28 ($d * i * j$)로** 보면 **($i, j$) 위치는 6-dim vector**로 생각할 수 있다.

    - 이를 `Grid of feature vector`라고 한다.
    - 출력을 **입력 공간 (28 * 28)과 동일한 공간**으로 생각하는 관점이다.

2. 결과로 나온 **6-dim에 대해 각 차원마다 28 * 28 grid**를 가지고 있는 것으로 생각할 수 있다.

    - 이 관점은 `Stack of 2D feature maps`이라고 한다.
    - 입력 이미지의 **한 부분이 `Filter`에 대응되는 정도**를 나타낸다.

마지막으로, 일반적으로 하나의 이미지만을 가지고 `Convolution Latey`를 사용하지 않고 `Batch size`만큼의 여러 이미지를 이용한다. 

- 입력의 맨 앞에 `Batch size`라는 dimension을 추가하고, **4차원 Tensor처럼 다룰 수 있다.**

- 이 경우 각 이미지에 대한 6개의 `Activation` 맵이 `Batch size`만큼 나온다.

- 즉, 총 `Output size` * `Batch size`만큼의 `Activation map`이 생긴다.

![alt text](image-197.png)

Batch까지 사용하는 것을 포함하여, 전체 `Convolution Layer`의 과정을 일반화하면 아래와 같다.
- $N$: Batch size
- $C_{in}$" Input size
- $C_{out}$: Output size
- $H$: Input iamge row size 
- $W$: Input image col size
- $K_w$: Filter col size
- $K_h$: Filter row size

![alt text](image-198.png)


## Stacking Convolutions

> **Input Image에 대해 하나의 `Convolution layer`만 적용하는 것이 아니라 여러 `Convolution Layer`를 적용할 수 있다.**

![alt text](image-199.png)

- 이전 `Convolution Layer`의 결과에 따라 다음 Layer에서 사용하는 `Layer`의 `Depth`가 다른 것을 확인할 수 있다.

- $b_1$은 오타로, 6이 되어야 한다.

- `Fully-connected layer`와 동일하게 중간에 숨겨진 부분은 `Hidden Layer`라고 부른다.

위 상황에서는 단순히 `Weight`를 이전 `Layer`에서 얻은 결과에 대해 `Convolution` 연산만 하고 있다.

이 상황처럼 **단순히 `Stacking`하면 `Convolution` 결과에 대해 또 다른 `Convolution`을 실행**한 것과 동일하다. 

`Convolution` 역시 **Linear operation**이기 때문에, 단순히 **`Stacking`만 하는 것**은 `Fully-connected layer`와 마찬가지로 **하나의 `Convolution Layer`를 사용하는 것과 동일**하다.

- `Fullt-connected layer`에서 `Activation function` 없이 단순히 `Stacking`만 하는 것은 다른 큰 `Weight`를 가진 하나의 `Fully-connected layer`를 사용하는 것과 동일하다.

해결하는 방법 역시 `Fully-connected layer`와 동일하다. `Layer` 사이에 **`ReLU`와 같은 `Activation function`을 사용하면 된다.**

![alt text](image-200.png)

## What do Convolution filters learn?

> **`Fully-connected layer`와 동일하게, `Convolution Layer`도 `Weight`를 시각화해서 Model이 어떤 것을 학습하는지 알아볼 수 있다.**

그 중, **첫 번째 Layer에서 사용되는 Weight**를 시각화해보자.

아래 예시는 `Convolution Neural Network`의 `AlexNet`이라는 모델의 첫 번째 `Weight`를 시각화한 것이다.

![alt text](image-201.png)

- 첫 번쨰 `filter`는 대부분 **방향성을 가진 edge**나 `색상 대비가 일어나는 부분`으로 이루어져 있다.

- `Grid of feature vector` 관점에서 보면, `Activation map`의 공간에서 특정 위치 ($i, j$)에 대응되는 Vector는 **각 filter에 이미지의 부분이 반응하는 정도**를 저장하고 있다.

## A closer look at spatial dimensions

입력 이미지에서 Filter를 한 칸씩 이동하면 `Activation map`의 크기가 어떻게 될 지 살펴보자.

![alt text](image-202.png)

일반적으로는 `input`: $W$ x $W$, `filter`: $K$ x $K$ 일 때, `output`은 ($W - K + 1$) x ($W - K + 1$) 이다.

하지만 이 경우 문제점이 있다. 

**한 Layer를 통과할 때마다, 한 행당 $K-1$ 개의 Pixel이 없어지게 된다.**

- 결국, 이 경우에는 **Input size에 의해 Network의 깊이가 제한**될 수 있다.

해결책은 `Padding`이라는 방법이다.

### Padding

`Input image`의 **size가 Layer를 통과할수록 너무 작아**지는 것을 방지하기 위해, **`Convolution` 연산 하기 전 `Input image`에 몇 개의 Pixel을 추가하는 방법**

![alt text](image-203.png)

- `Padding`의 종류가 여러 가지 있지만, 여기서 사용한 방법은 `Zero padding`이다.

`Padding`은 $P$라는 `Hyperparameter`를 새롭게 추가해야 한다.
- $P$는 **한쪽 변에 추가할 Pixel의 개수**를 의미한다.

가장 많이 사용하는 $P$ = $K - 1$이다.
- 이 방법은 `Same padding`이라고 부른다.

- `Layer`를 통과해도 입력 이미지의 크기가 고정되어 `Layer`가 거듭되도 size 추론이 쉽기 때문이다.

`Padding`까지 사용한다면, 필요한 `Hyperparameter`는 아래와 같다.

![alt text](image-204.png)

## Receptive Fields

> **Output tensor가 Input Tensor의 어느 부분을 참조하는지를 나타낸다.**

![alt text](image-205.png)

- 여기서 $K$는 kernel의 한 dimension의 크기를 의미한다.

- 직전 Layer에 대한 `Receptive field` 즉, `Receptive Field in the previous layer`는 직전 `Filter`의 크기이다.

`Receptive Field`는 하나의 Layer에 대해서가 아니라 여러 Layer에 대해서도 나타낼 수 있다.

![alt text](image-206.png)
  
- $L$이 Layer의 개수라고 할 때, **현재 시점으로부터 $L$번째 Layer 뒤에 있는 `Receptive field`의 크기**는 $1 + L * (K-1)$ 이다.

- `Receptive field in the input image`는 **해당 뉴런이 `Input image`에서 몇 개의 Pixel을 보고있는지**를 의미한다.

하지만, 여기서 문제점이 있다.

만약, `Input image`가 엄청 나게 큰 경우, **하나의 뉴런이 `input image`의 전체 영역에 대한 정보를 담고있으려면 굉장히 많은 `Layer`가 필요하다.**

큰 `Receptive Field`를 얻기 위해서도 많은 `Layer`가 필요하다.

해결법은 `Stride`라는 `Hyperparameter`를 추가하여 `Input image` 또는 `Convolution layer output`을 `Downsampling` 하는 것이다.

## Strided Convolution

> **이전까지 배웠던 `Convolution Layer`는 `Stride = 1`인 `Convolution Layer`이다. 1보다 큰 `Stride`를 갖는 `Convolution Layer`를 `Strided Convolution Layer`라고 한다.**

`Stride = 2`인 경우를 살펴보자.

![alt text](image-207.png)

- `Stride`만 바뀌었는데 `Output size`가 줄어든 것을 확인할 수 있다.

- `Output size`가 줄어들기 때문에 `Downsampling`이라고 한다.

- `Receptive field`를 구해보면, **기존에 비해 2배 커진 것**을 확인할 수 있다. 

`Hyperparameter`에 `Stride` 크기를 의미하는 $S$가 추가되고, `Output size`를 구하는 공식도 바뀐다.

![alt text](image-208.png)

- 나누는 부분이 나누어 떨어지지 않을 때, 반올림할 수도 있고 나머지를 버릴 수도 있다.

## Example

아래와 같은 상황이 있다고 가정하자.

![alt text](image-209.png)

`Padding`과 `Stride`까지 고려한 `Output size` 공식을 적용하면 아래와 같이 구할 수 있다.

![alt text](image-210.png)

`Learnable Parameter`의 개수는 **`Filter`의 Element의 총 개수**와 동일하므로 아래와 같이 구할 수 있다. 

![alt text](image-211.png)

`Multiply-add Operation`은 `Input image`에서 **`Filter size`와 동일한 크기의 두 Tensor 끼리의 내적**이고, **해당 내적의 계산 횟수는 `Output size`와 동일**하므로, `Multiply-add Operation`의 개수는 아래와 같이 구할 수 있다.

![alt text](image-212.png)


### 1 x 1 Convolution

> **`Filter`의 크기가 1 x 1 x depth인 `Convolution`**

![alt text](image-213.png)

`Output`을 `grid of feature vector` 관점으로 보면, **`1 x 1 filter`는 각 `Feature vector`에 대해 독립적으로 동작한다.**

- 이 관점에서 보면, 64-dim $x$를 32-dim $x'$ 으로 바꾸는데 이는 `Fully-connected layer`와 완전히 같은 역할을 수행한다.

- 따라서, `1×1 Convolution`은`Convolution layer` 안에서 위치별로 `MLP(Fully Connected Layer)`를 적용하는 구조이기 때문에 이를 `Network in Network`라고 부른다.

`1 x 1 Convolution`은 **Feature space 축소 / 확장**, **channel간의 정보 결합**, **연산량 감소** 등의 이유로 사용된다.

#### 1 x 1 Convolution vs MLP

> **둘은 해석에 따른 차이가 존재한다.**

`1 x 1 Convolution`은 Network 내에서 `Channel Space`의 차원을 다음 Layer에 맞게 변경하는 등, **차원의 수를 변경**하기 위해 주로 사용된다.

- Network내에서 일종의 `Adapter`로 사용된다.

`MLP`는 `Vector`를 입력으로 받거나 `Flatten`하여 Vector 형태의 출력을 생성한다.

- 즉, 입력 공간 자체를 파괴한다.
- Class에 따른 Score를 생성해야 할 때 주로 사용된다.

## Summary of 2D convolution

![alt text](image-214.png)

## Other type of Convolution

> **지금까지 본 Convolution은 전부 2D Convolution이다. 다른 유형의 Convolution에 대해서도 알아보자.**

### 1D Convolution

![alt text](image-215.png)

- 오른쪽으로 이동하며 `Convolution` 연산을 수행

- 오디오 데이터나 텍스트 데이터에서 주로 사용된다.

### 3D Convolution

![alt text](image-216.png)

- 4D를 표현할 수 없어 3D grid에서 **$C_{in}$-dim feature vector**로 표현했다.
- 3D 데이터를 처리하는데 주로 사용된다.

## Pytorch Convolution Layer

`Conv2d`라는 함수를 통해, `2D Convolution layer`를 구현할 수 있다.

![alt text](image-217.png)

`1D Convolution layer`와 `3D Convolution layer`를 구현하는 함수도 지원한다.

![alt text](image-218.png)

# 2. Pooling Layers

> **Input값의 집합을 받아 하나의 출력 값으로 줄여주는 `Downsampling`을 하는 방법**

`Convolution Layer`에서 사용한 `Downsampling` 방법은 **`Kernel size`를 늘리는 방법** 그리고 **`Stride`를 크게 하는 방법**이 있었다.

두 방법 외에도 `Pooling`을 통해 `Downsampling`을 할 수 있다.

![alt text](image-219.png)

`Pooling`의 방법을 지정하는 `Pooling function`이 `Hyperparameter`로 사용된다.

![alt text](image-221.png)

- `Stride`와 `Kernel size`에 따라 **Pooling할 범위**가 정해진다.

## Max Pooling

> **가장 많이 사용되는 Pooling function**

`2 x 2 Max Pooling`을 예시로 살펴보자.

![alt text](image-220.png)

- **`Kernel size`와 `Stride`를 같게 설정**하면, Input에서 `Filter`가 적용되는 위치가 겹치지 않는다.
    
    - 이 때문에 일반적으로 같게 설정한다.

- `Pooling function`으로 `Max function`을 사용한다.

`Downsampling`을 위해 `Convolution layer`를 사용할 수 있는데 **굳이 `Pooling`을 사용하는 이유**는 아래와 같다.

1. `Convolution layer`의 `Weight`처럼 **`Learnable Paramter`가 필요 없다.**

2. `Max pooling`의 경우 특정 영역에서 최댓값만 가져오기 때문에, **`Input image`가 일부 수정되더라도 최댓값은 거의 변하지 않는다는 믿음**을 기반으로 **어느 정도의 `Invariance` (불변성)을 보장**할 수 있다.

    - 가장 강한 `Activation` (반응)만을 전달하기 때문이다.

### Convolution vs Pooling

___
`Convolution`은 **`weight`와 `input patch` 간의 `dot product`을 통해 출력을 계산**한다.

하지만, `Pooling`은 **학습 가능한 가중치 없이, 해당 영역에 고정된 연산(max, avg 등)을 적용하여 출력**을 만든다.

이처럼 `Pooling`은 **정해진 영역에 대해 항상 동일한 연산**을 수행하기 때문에, **출력값이 규칙적으로 동기화(synchronized)**된다.

- 출력 값이 **예측 가능하고 정규화**된 방식으로 정해진다


### Averaging Pooling

___

> **Max Pooling처럼 자주 사용되는 Pooling 기법 중 하나이다.**

- **입력 집합의 평균값**을 사용한다.

## Summary

![alt text](image-222.png)

# Convolutional Network

`Convolution layer`와 `Pooling`을 이용하면, **전통적인 `Convolutional Network`를** 만들 수 있다.

고전적인 `Convolutional Network`의 구성은 다음과 같다.

![alt text](image-223.png)

`LeNet-5`를 예시로 하여 더 자세하게 알아보자.

## Example: LeNet-5

데이터셋은 `CIFAR-10`을 사용한다고 가정한다.

먼저 `input`에 대해 `convolution` 연산을 진행하고, `ReLU function`을 적용한다.

![alt text](image-224.png)

`Max pooling`을 적용한다.

![alt text](image-225.png)

위 두 작업을 한 번 더 반복하면 아래와 같은 상황이 된다.

![alt text](image-226.png)

이제 `Convolution layer`를 통과해서 나온 `Output`을 `Fully connected layer`에 넣기 전에, **Flatten 해야 한다.**

![alt text](image-227.png)

Flatten한 이후에는 `ReLU function`을 Activation function으로 사용하는 `Fully connected layer`에 통과시킨다.

마지막 Layer는 10개 Class에 대한 Score를 생성하기 위한 Layer이다.

![alt text](image-228.png)

전체 과정을 진행하고 난 후에 과정을 정리해보면 아래와 같다.

![alt text](image-229.png)

- 일반적으로 Network를 통과하며 **Spatial size ($H, W$)는 감소한다.**

- 일반적으로 Network를 통과하며 **Channel size는 늘어난다.**

- `Volume` = $C \cdot H \cdot W$는 유지되거나 더 커진다.
  - `Spatial size`의 감소를 `Channel size`를 늘림으로써 커버하여 **총 정보량은 보존**될 수 있도록 한다. 

**Question) Max Pooling이 일종의 비선형성을 도입하는데 왜 ReLU까지 사용하는가?**

**Answer)** 비선형성만을 고려하면 필요하지 않을 수도 있지만, Modern model에서는 **Max Pooling과 상관없이 ReLU를 사용**하는 추세이다. 

위 고전적인 `Convolutional Network`는 문제점이 있다. 

위 방법에 따라 설계하다 보면, **Network는 점점 더 Deep해지는데 `Training`과정에서 Model의 수렴성을 보장할 수가 없게** 된다.
- 이 문제를 해결하기 위해 `Normalization`을 사용한다.


# 3. Normalization

> **Deep Network에서 더 쉽게 훈련할 수 있도록 추가하는 Layer**

## Batch Normalization

> **가장 일반적인 Normalization 방법으로, 이전 Layer의 Output을 받아서 해당 Output이 평균이 0, 표준 편차가 1인 분포가 되도록 Normalization 하는 방법**

이 방법을 사용하는 이유는 `Internal Covariate shift`를 줄이기 위함이다.

- 모델을 Training할 때, 한 Layer는 이전 Layer의 Output을 보고 있다.
  
- 모든 다른 `Weight Matrix`가 동시에 훈련되기 떄문에 이전 `Layer`의 **`Weight Matrix`가 최적화 과정에서 변경되면 다음 레이어의 출력 분포도 최적화 과정에서 변경**된다.

- 이 때문에 두 번째 Layer가 학습시마다 **다른 분포의 입력을 받는다**는 것을 `Internal Covariate Shift`라고 한다.

- `Internal Covariate Shift`는 최적화 측면에서 좋지는 않다.

따라서 **특정 Layer에서의 입력의 분포가 변하지 않도록** **평균이 0, 표준편차가 1인 분포를 따르도록 강제**하기 위해서 `Batch Normalization`을 사용한다.

아래 수식으로 정규화를 한다.

![alt text](image-230.png)

- $x^{(k)}$: 전체 데이터셋에서의 Sample 집합

- Sample에 대해 **평균을 빼고 표준편차로 나눈다.**

- 이 함수는 **Differentiable function**이다.

- 즉, **미분이 가능하므로 Neural Network 내부에 넣을 수 있다.**

실제 입력 $x$가 $N$: input size, $D$: input dimension 크기로 이루어졌다고 했을 때, 했을 때, 아래와 같이 평균과 표준편차를 계산할 수 있다.

![alt text](image-231.png)

- Batch 내에서의 **전체 이미지에 대한 평균, 표준편차를 계산**해야 하기 때문에, `Per-channel mean, std`를 구하는 것을 확인할 수 있다.

**문제점 1)** 우리가 최적화을 효과적으로 하기 위해 입력 $x$가 **평균을 0, 표준편차를 1인 분포를 따르도록 강제하는 것은 생각보다 큰 제약**일 수 있다.

따라서 정규화 이후에 Learnable `scale` and `shift parameters`를 추가하는 작업을 해준다.
- 두개 모두 $D$-dimension이다.

두 `Parameter`는 `Normalizationed output` $x_{hat}$에 적용된다.

![alt text](image-232.png)

$y_{i, j}$를 확인하면 `Network`가 스스로 `Scale`과 `shift`를 학습하여 입력 $x$를 **평균:0, 표준편차:1이 아닌 원하는 평균과 표준편차를 따르는 분포를 따르도록 만들 수 있다.**

**문제점 2)** 아래에 보이는 현재 평균과 표준편차 계산 방식은 `Batch` **내에서** **서로의 이미지가 독립적이지 못 하고 다른 이미지에 대해 의존적이라는 문제점**을 가진다는 것을 확인할 수 있다.

![alt text](image-233.png)

- 기존에는 **Batch의 일부 이미지만 Input으로 들어와도 바로 Test를 수행하거나 결과**를 낼 수 있었다. 

- `Batch Normalization`을 사용하면, Batch 안에 들어있는 **모든 이미지에 대해 평균을 내야하므로 Batch의 모든 이미지를 받아야 Test를 수행**하거나 결과를 낼 수 있다.

- 이는 Real-time service 등에서 치명적이다.

이를 해결하기 위해, 평균과 표준편차를 다른 방식으로 구한다.

![alt text](image-234.png)

**`Training` 과정에서 본 해당 Layer에서의 모든 평균과 표준편차를 `Exponential Averaging`하여 상수처럼 사용한다.**
- Test시에는 그냥 이 상수를 사용한다.

이렇게 Layer에서의 **모든 평균과 표준편차를 상수**처럼 만들면, Testing시 적은 시간이 소요되는 것 외에도 다른 장점이 존재한다.

평균과 표준편차가 상수라면, **`Normalization` 과정은 `Linear`한 과정이 되고 `Convolution`도 `Linear`하기 때문에 두 과정을 따로 하는 것이 아니라 하나의 과정으로 합칠 수 있다.**

- **두 `Linear Operation`은 하나의 `Linear Operation`으로 합칠** 수 있다.

- 결국, **Test time의 Overhead가 0**이 된다.

___

`FC Network`에서 `Running Average` 방법을 적용한 `Batch Normalization`이 어떻게 동작하는지 알아보자.

![alt text](image-235.png)

- 입력이 Vector이기 때문에, **Batch에 대해서만 평균**내면 된다.

`Convolutional Network`에서 위 방법을 적용한 `Batch Normalization`이 어떻게 동작하는지 알아보자.

![alt text](image-236.png)

- **`Batch`와 `Spatial Dimension`에 대해 평균**을 내야한다.

**일반적으로 `Batch Normalization`은 `FC / Convolutional Layer` 바로 뒤, `Activation function` 적용 이전에 사용하는 것이 일반적이다.**

![alt text](image-237.png)

### Batch Normalization의 장점

아래와 같은 장점이 있다.

![alt text](image-238.png)

그래프로 확인했을 때, `Batch Normalization`을 적용한 모델이 훨씬 빠른 것을 확인할 수 있다.

![alt text](image-239.png)

### Batch Normalization의 단점

![alt text](image-240.png)

- `Training time`과 `Testing time`에 **다른 동작을 할 수도 있다는 것이 가장 큰 문제**이다.

 - `Training` 때 계산한 `Running Average`를 사용하기 때문에, `Test data`와 `Training data`가 다르다면 `Running Average`가 적합하지 않을 수 있다.  

- **학습 / 테스트 데이터가 불균형한 경우**에 정규화 제약 조건을 두는 것이 오히려 비효율적일 수 있다.

- Batch의 크기나 구성에 따라 결과가 달라질 수 있다.

## Layer Normalization

> **`Batch Normalizatino`이 "`Training time`과 `Testing time`에 "다른 동작을 할 수도 있다"라는 문제를 해결하기 위해 사용한다.**

![alt text](image-241.png)

가장 큰 차이점은 `Batch dimension`에 대해 평균 및 표준편차를 계산하는 것이 아니라, `Feature dimension`에 대해 평균 및 표준편차를 계산한다는 점이다.

- 이미지간의 독립성이 보장되므로 `Training`과 `Test` 때 다른 결과를 유발할 가능성이 사라진다.

## Instance Normalization

> **Batch dimension에 대해서는 평균을 구하지 않고, Spatial dimension에 대해서만 평균을 구하는 방법**

![alt text](image-242.png)

- Batch dimension에 대해 평균 / 표준편차를 구하지 않기 때문에, **Train과 Test시 동일하게 동작**한다.

## Group Normalization

> **Channel 차원을 여러 그룹으로 나누고, 그룹마다 평균 / 표준편차를 구하는 방법**

## Comparision of Normalization Layers

![alt text](image-243.png)
