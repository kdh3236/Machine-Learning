우리는 원하는 특성만을 선택하여 모델의 복잡도를 줄이고 Overfitting 을 피할 수 있다.

**원하는 특성 선택을 통해 dimensionality reduction 을 이뤄낼 수 있다.**
- **feature selection**
- **feature extraction**

# Sequential feature selection algorithm
> **Greedy search algorithm 으로 초기 $d$ 차원의 공간을 $k<d$ 인 $k$ 차원의 feature subset 으로 축소한다.**

## Sequential Backward Selection (SBS) 
> **가장 전통적인 Sequential feature selection**

어떤 특성을 제거하기 전후의 모델 성능 차이 (전 - 후)를 기준 함수로 하고, 각 단계에서 기준 값이 가장 큰 특성을 제거한다.

1. 알고리즘을 $k=d$ 로 초기화한다. $d$는 데이터의 차원이다.
2. 제거했을 때, 기준 함수가 가장 커지는 특성 $x$를 찾는다. 
	
    - 각 단계에서 $k$ 개로 만들 수 있는 특성의 선형 조합을 가지고 모델을 학습시켜, 가장 정확도가 높은 특성 조합을 선택한다. 
3. Feature set에서 $x$를 제거한다. $k = k - 1$
4. $k$ 가 목표로 하던 특성 개수가 되면 종료한다.

### 파이썬으로 SBS 구현
```python
from sklearn.base import clone
from itertools import combinations
import numpy as np
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

class SBS:
	# estimator는 모델을 학습시킬 학습 알고리즘이다.
    # 사이킷런 라이브러리의 클래스를 사용해도 된다.
    def __init__(self, estimator, k_features, scoring=accuracy_score, test_size=0.25, random_state=1):
        self.scoring = scoring
        self.estimator = clone(estimator)
        self.k_features = k_features
        self.test_size = test_size
        self.random_state = random_state

    def fit(self, X, y):
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=self.test_size, random_state=self.random_state)

        dim = X_train.shape[1] # k
        # 현재 사용 중인 모든 특성의 인덱스를 저장
        self.indices_ = tuple(range(dim))
        # 특성명을 배열로 저장
        self.subsets_ = [self.indices_]
        score = self._calc_score(X_train, y_train, X_test, y_test, self.indices_)
           
        self.scores_ = [score]
		
        # 목표하는 K가 될 때 까지
        while dim > self.k_features:
            scores = []
            subsets = []
			
            # combinations(self.indices_, r=dim - 1)
			# → 현재 dim개의 특성 중 하나를 제거한 모든 조합을 생성
            # indices_ 특성 중, dim-1 개의 특성을 선택
            for p in combinations(self.indices_, r=dim - 1):
                score = self._calc_score(X_train, y_train, X_test, y_test, p)
                scores.append(score)
                subsets.append(p)
			
            # scores 배열 중 가장 높은 값의 인덱스
            best = np.argmax(scores)
            self.indices_ = subsets[best]
            self.subsets_.append(self.indices_)
            dim -= 1
            self.scores_.append(scores[best])
        
        self.k_score_ = self.scores_[-1]
        return self
    
    def transform(self, X):
        return X[:, np.array(self.indices_)]
	
    # Model 을 학습시킨다.
    def _calc_score(self, X_train, y_train, X_test, y_test, indices):
        self.estimator.fit(X_train[:, indices], y_train)
        y_pred = self.estimator.predict(X_test[:, indices])
        return self.scoring(y_test, y_pred)
```

## Sequential Forward Selection (SFS)
1. 특성을 선택하지 않은 상태에서 시작 

2. 남아있는 특성 중 하나를 추가했을 때 성능이 가장 좋은 특성을 선택하고 추가
3. 위 과정을 원하는 개수(k_features)에 도달할 때까지 반복

# 사이킷런을 이용한 구현
학습 알고리즘으로 KNN을 사용한 Sequential feature selection 을 구현해보자
- **SequentialFeatureSelector () class 의 direction = parameter의 값에만 차이가 있다.**
### SFS
```python
from sklearn.datasets import load_iris
from sklearn.neighbors import KNeighborsClassifier
from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.model_selection import train_test_split

# 데이터 로드
X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)

# Sequential Forward Selection (SFS)
sfs = SequentialFeatureSelector(
    KNeighborsClassifier(n_neighbors=3),  # 사용할 모델
    n_features_to_select=2,  # 선택할 특성 개수
    direction="forward",  # Forward Selection 수행
    scoring="accuracy",  # 평가 기준
    cv=5  # 5-fold 교차 검증
)

sfs.fit(X_train, y_train)

# 선택된 특성 인덱스 출력
print("선택된 특성 인덱스:", sfs.get_support(indices=True))

# 선택된 특성만 사용하여 새로운 데이터셋 만들기
X_train_selected = sfs.transform(X_train)
X_test_selected = sfs.transform(X_test)

# 선택된 특성으로 KNN 학습 및 평가
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train_selected, y_train)
accuracy = knn.score(X_test_selected, y_test)

print("최종 모델 정확도:", accuracy)

```
### SBS

```python
from sklearn.datasets import load_iris
from sklearn.neighbors import KNeighborsClassifier
from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.model_selection import train_test_split

# 데이터 로드
X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)

# Sequential Forward Selection (SFS)
sfs = SequentialFeatureSelector(
    KNeighborsClassifier(n_neighbors=3),  # 사용할 모델
    n_features_to_select=2,  # 선택할 특성 개수
    direction="backward",  # Forward Selection 수행
    scoring="accuracy",  # 평가 기준
    cv=5  # 5-fold 교차 검증
)

sfs.fit(X_train, y_train)

# 선택된 특성 인덱스 출력
print("선택된 특성 인덱스:", sfs.get_support(indices=True))

# 선택된 특성만 사용하여 새로운 데이터셋 만들기
X_train_selected = sfs.transform(X_train)
X_test_selected = sfs.transform(X_test)

# 선택된 특성으로 KNN 학습 및 평가
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train_selected, y_train)
accuracy = knn.score(X_test_selected, y_test)

print("최종 모델 정확도:", accuracy)
```