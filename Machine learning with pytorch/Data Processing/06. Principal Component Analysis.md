# PCA
> **고차원에서 저차원으로의 차원 축소 기법으로 PCA 라고도 한다.**
**데이터에 대해 가장 많은 정보를 남기기 위한 방법이며, 이를 위해 분산이 큰 것을 선택한다.**

**목표는 행렬을 낮은 차원의 행렬로 변환한 후, 분산이 가장 큰 k개를 선택하여 최대한 많은 정보를 남기는 것이다.**

-  분산이 커 데이터가 많이 퍼져 있는 방향일수록 데이터가 더 다양한 값을 가지므로, 더 많은 정보를 담고 있다고 볼 수 있다.
- 선택된 각 주성분들은 x, y, z, ...  축과 평행하지 않을 수도 있다.
## 고윳값 분해
📌 **고윳값 분해의 핵심** 
어떤 정방행렬 𝐴에 대해 **고윳값 𝜆과 고유벡터 𝑣**는 다음 관계를 만족한다.
- $Av = \lambda v$

- **즉, 정방행렬 A에 대해, 방향을 바꾸지 않고 크기만 변화시키는 vector $v$ 가 존재한다.**
## 과정
1. **d차원 데이터셋을 표준화 전처리한다.**
	
    - PCA는 분산을 다루기 때문에 데이터 스케일에 영향을 크게 받는다.
2. **Covariance matrix 를 만든다.**
	
    - $N$ x $N$ 행렬이고 , $C^T = C$ 이다.
3. **Covariance matrix 를 eigenvector와 eigenvalue 로 분해한다.**
4. **Eigenvalue 를 정렬한다.**
5. **고윳값이 가장 큰 $k$ 개의 eigenvector를 선택한다.**

	- 고윳값은 해당 **고유벡터(주성분)가 나타내는 데이터의 분산 크기**를 나타낸다.
    - 고윳값이 크다는 것은 **데이터가 그 주성분 방향으로 더 많이 퍼져 있다는 것이고, 이는 그 방향에 더 많은 정보가 있다**는 뜻이다.
6. **최상위 $k$ 개의 eigenvector 로 Projection matrix W를 만든다.**
7. **Projection matrix W 를 사용하여 $d$ 차원 입력 데이터셋 X를 새로운 $k$ 차원의 특성 부분 공간으로 변환한다.**

![](https://velog.velcdn.com/images/kvvon/post/c854ad32-0ff8-426e-9d7e-3637dd30aaa8/image.png)![](https://velog.velcdn.com/images/kvvon/post/5197ac37-4291-4958-b753-78976a70692e/image.png)![](https://velog.velcdn.com/images/kvvon/post/ced5741f-27e3-4a73-9765-682f1154634b/image.png)![](https://velog.velcdn.com/images/kvvon/post/f81d3414-d6da-43c0-9a76-f9accf697a2c/image.png) ![](https://velog.velcdn.com/images/kvvon/post/d7b3dea5-1e8c-41ff-8618-476e0b7208c1/image.png)

## PCA 파이썬으로 구현하기
![](https://velog.velcdn.com/images/kvvon/post/ab164d3a-eb43-48bf-8c24-46b7c4db054c/image.png) ![](https://velog.velcdn.com/images/kvvon/post/ad612d00-1553-44a3-885e-fdeec00b8d89/image.png) ![](https://velog.velcdn.com/images/kvvon/post/85ac1d24-7cb9-4904-afd7-618640748e6d/image.png)![](https://velog.velcdn.com/images/kvvon/post/de9afb30-3fbb-469a-ab43-2cd48ecbf54a/image.png)

- 결과를 보면 **X축 (제1주성분) 방향으로 데이터가 비교적 넓게 펴져있는 것**을 확인할 수 있다. 

### Explained variance ratio
> **특성 고유값을 전체 고윳값의 합으로 나눈 것**

- $k$ 개의 특성을 선택하기 전에 특정 Eigenvalue 가 차지하는 비율을 확인할 수 있다.

## PCA 사이킷런으로 구현하기
```python
from sklearn.decomposition import PCA
# 주성분 개수 지정
pca = PCA(n_components=2)

# 차원 축소
X_train_pca = pca.fit_transform(X_train_std)
X_train_pca = pca.transform(X_test_std)

# 전체 주성분 모두 사용
pca = PCA(n_components=None)
X_train_pca = pca.fit_transform(X_train_std)

# 전체 주성분의 Explained variance ratio를 확인하자
print(pca.explained_variance_ratio_)

# n_components = 0 ~ 1 사이 실수
# Cumulative Explained variance ratio가 실수값 이상이 될 때까지 진행
pca = PCA(n_components=0.8)
```
- 사이킷런의 PCA 클래스는 **Singular decomposition 방식으로 고윳값 분해를 진행한다**.

```python
from sklearn.decomposition import IncrementalPCA
ipca = IncrementalPCA(n_components = 9)

for batch in range(len(X_train_std)//25+1):
	X_batch = X_train_std[batch*25:(batch+1)*25]
	ipca.partial_fit(X_batch)
```

- 전체 데이터를 한 번에 처리하는 PCA와 달리, **IncrementalPCA는 작은 배치 단위로 데이터를 업데이트하면서 주성분을 학습한다**.
- 메모리 절약이 가능하다.

## 특성 기여도(Loading) 확인
> **Loading(로딩 행렬)은 각 원본 특성이 주성분에 얼마나 기여하는지를 나타내는 값이다.이는 주성분을 해석하는 중요한 지표로, 특정 주성분이 어떤 원본 특성과 관련이 있는지를 파악할 수 있다.**

![](https://velog.velcdn.com/images/kvvon/post/8dd9b95e-821a-4367-9105-68f73fda7ae8/image.png) ![](https://velog.velcdn.com/images/kvvon/post/e76915d5-30b5-4b4f-b1fa-cd8f66be3c25/image.png)![](https://velog.velcdn.com/images/kvvon/post/ae8a1638-aa22-4909-b53f-6bc47f01f074/image.png) ![](https://velog.velcdn.com/images/kvvon/post/65dd8467-6dc7-4114-8837-ca3f9ece29f3/image.png) ![](https://velog.velcdn.com/images/kvvon/post/08c98d25-8940-4342-9933-2575f026ada5/image.png)

- **Eigen vector 자체가 Covariance matrix로부터 도출되었기 때문**에 가능하다고 생각하면 편하다.

- **양의 상관관계**: 정비례
- **음의 상관관계**: 반비례

### 파이썬으로 구현
```python
loadings = eigen_vecs * np.sqrt(eigen_vals)
```

### 사이킷런으로 구현
```python
from sklearn.decompositino import PCA

sklearn_loadings = pca.components_.T * np.sqrt(pca.explaned_variance_)
```

