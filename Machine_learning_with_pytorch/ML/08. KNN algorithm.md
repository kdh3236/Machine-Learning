# K-NN Algorithm 
> **K-Nearest Neighbor algorithm 이라고도 하며, Lazy learner 의 전형적인 예시**

- **Lazy learner:** Loss / Discriminative function을 이용하여 학습을 하는 것이 아니라, **훈련 데이터셋 자체를 메모리에 저장하고 테스트 데이터셋을 훈련 데이터 셋과의 유사도를 기반으로 분류한다.**
	
- 보통 이런 경우를, **Instance-based model** 이라고 하며 **Lazy learner는 Instance-based model의 특별한 경우이다.**

![](https://velog.velcdn.com/images/kvvon/post/bab49134-d9f1-43fd-8edc-8d23978439b9/image.png)

## K-NN Algorithm의 과정
1. 새로운 데이터가 주어졌을 때, 해당 데이터로부터 모든 훈련 데이터들과의 거리를 구한다.
2. 거리가 가까운 K개의 훈련 데이터들을 선택한다.
3. 선택된 K개의 훈련 데이터 중, 특정 클래스 레이블에 해당하는 훈련 데이터가 가장 많은 것을 새로운 데이터의 클래스 레이블로 할당한다.

## K-NN Algorithm의 단점
1. 만약 K가 짝수라면, 서로 다른 두개 이상의 클래스 레이블이 동점인 경우가 발생할 수 있다.
	
    - 이 경우, 거리가 가장 가까운 데이터의 클래스 레이블을 선택하긴 한다.
2. 훈련 데이터셋이 커질수록 시간이 선형적으로 증가한다.
3. 새로운 데이터와 훈련 데이터가 너무 멀면 결과에 좋지 않다.
	
    - **Curse of dimensionality:** 차원이 늘어나면, 아무리 가까운 훈련 데이터라도 좋은 추정값을 만들기엔 너무 멀리 떨어져 있다.
    - Overfitting을 유발한다.
    - 차원 축소, 특성 선택이 Overfitting 예방에 도움이 된다.
    
    
# 사이킷런으로 K-NN Algorithm 구현

``` python
from sklearn.neightbors import KNeighborsClassifier

# k = n_neightbors 
knn = KNeightborsClassifier(n_neightbors=5, p=2, metric='minkowski')

# 거리를 다루기 때문에 일반적으로 표준화 해주는 것이 좋음
knn.fit(X_train_std, y_train)
```
**KNeightborsClassifier(n_neightbors=5, p=2, metric='minkowski')** ![](https://velog.velcdn.com/images/kvvon/post/95f83244-fa24-4f5e-867d-7ddf3f39817f/image.png) ![](https://velog.velcdn.com/images/kvvon/post/80c86428-c958-407c-916b-065fe403e956/image.png) ![](https://velog.velcdn.com/images/kvvon/post/a1ca94ef-9f0a-4c5c-baae-b326ea0cf07a/image.png)
> **사이킷런에서 K-NN은 Parameter를 통해 정해진 거리 측정 방식으로 새로운 데이터와 훈련 데이터들간의 거리를 측정하여 가까운 K개를 이용해 클래스를 판별하는 방식으로 동작한다.**

# Nearest neightbors decision boundary theorem - Cover & Hart(1967) 
> **이상적인 상황에서 KNN의 오차율은 베이즈 오차율(Bayes Error Rate)의 2배 이하로 수렴함을 증명하는 이론**

![](https://velog.velcdn.com/images/kvvon/post/214efa7c-09b2-43dd-8f0d-1cddbb2b5809/image.png) ![](https://velog.velcdn.com/images/kvvon/post/005b2581-8ac8-4f2a-bf8a-3c3f5ab67fc5/image.png)

1. **K=1일 때, 오차율은 베이즈 오차율의 최대 2배까지 가능하다.**

2. **k가 커지면 오차율이 베이즈 오차율($P^*$)에 가까워지지만, 너무 크면 Overfitting이 발생한다.**

3. **KNN의 오차율은 확률적으로 ($P^* + C\cdot\frac{1}{\sqrt{k}}$)로 근사할 수 있다.**