{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n",
    "\n",
    "## Frontier Model APIs\n",
    "\n",
    "In Week 1, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
    "\n",
    "Today we'll connect with the APIs for Anthropic and Google, as well as OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Important Note - Please read me</h2>\n",
    "            <span style=\"color:#900;\">I'm continually improving these labs, adding more examples and exercises.\n",
    "            At the start of each week, it's worth checking you have the latest code.<br/>\n",
    "            First do a <a href=\"https://chatgpt.com/share/6734e705-3270-8012-a074-421661af6ba9\">git pull and merge your changes as needed</a>. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/><br/>\n",
    "            After you've pulled the code, from the llm_engineering directory, in an Anaconda prompt (PC) or Terminal (Mac), run:<br/>\n",
    "            <code>conda env update --f environment.yml</code><br/>\n",
    "            Or if you used virtualenv rather than Anaconda, then run this from your activated environment in a Powershell (PC) or Terminal (Mac):<br/>\n",
    "            <code>pip install -r requirements.txt</code>\n",
    "            <br/>Then restart the kernel (Kernel menu >> Restart Kernel and Clear Outputs Of All Cells) to pick up the changes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Reminder about the resources page</h2>\n",
    "            <span style=\"color:#f71;\">Here's a link to resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Setting up your keys\n",
    "\n",
    "If you haven't done so already, you could now create API keys for Anthropic and Google in addition to OpenAI.\n",
    "\n",
    "**Please note:** if you'd prefer to avoid extra API costs, feel free to skip setting up Anthopic and Google! You can see me do it, and focus on OpenAI for the course. You could also substitute Anthropic and/or Google for Ollama, using the exercise you did in week 1.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api  \n",
    "\n",
    "### Also - adding DeepSeek if you wish\n",
    "\n",
    "Optionally, if you'd like to also use DeepSeek, create an account [here](https://platform.deepseek.com/), create a key [here](https://platform.deepseek.com/api_keys) and top up with at least the minimum $2 [here](https://platform.deepseek.com/top_up).\n",
    "\n",
    "### Adding API keys to your .env file\n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "Afterwards, you may need to restart the Jupyter Lab Kernel (the Python process that sits behind this notebook) via the Kernel menu, and then rerun the cells from the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for google\n",
    "# in rare cases, this seems to give an error on some systems, or even crashes the kernel\n",
    "# If this happens to you, simply ignore this cell - I give an alternative approach for using Gemini later\n",
    "\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AIzaSyAX\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "425ed580-808d-429b-85b0-6cba50ca1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the set up code for Gemini\n",
    "# Having problems with Google Gemini setup? Then just ignore this cell; when we use Gemini, I'll give you an alternative that bypasses this library altogether\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician?\n",
      "\n",
      "Because she found him too mean!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-4o-mini', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician?\n",
      "\n",
      "Because she found him mean, but he said she was just being too standard!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12d2a549-9d6e-4ea0-9c3e-b96a39e9959e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to the bar?\n",
      "\n",
      "Because they heard the drinks were on the house!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1-nano - extremely fast and cheap\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1-nano',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the logistic regression model?\n",
      "\n",
      "Because it just couldn’t commit!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96232ef4-dc9e-430b-a9df-f516685e7c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I’ve got a joke about overfitting… but I’m afraid it won’t generalize beyond this dataset.\n"
     ]
    }
   ],
   "source": [
    "# If you have access to this, here is the reasoning model o4-mini\n",
    "# This is trained to think through its response before replying\n",
    "# So it will take longer but the answer should be more reasoned - not that this helps..\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='o4-mini',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do data scientists prefer nature hikes?\n",
      "\n",
      "Because they love to go on random walks, and there's always a chance they'll find the perfect tree! 🌳\n",
      "\n",
      "*Plus, it's the only time their models can actually touch grass.*\n"
     ]
    }
   ],
   "source": [
    "# Claude 4.0 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do data scientists prefer dark chocolate?\n",
      "\n",
      "Because it has less noise and a higher signal-to-cocoa ratio! \n",
      "\n",
      "Plus, they heard milk chocolate leads to overfitting... in their jeans! 📊🍫"
     ]
    }
   ],
   "source": [
    "# Claude 4.0 Sonnet again\n",
    "# Now let's add in streaming back results\n",
    "# If the streaming looks strange, then please see the note below this cell!\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e17bc-cd46-4c23-b639-0c7b748e6c5a",
   "metadata": {},
   "source": [
    "## A rare problem with Claude streaming on some Windows boxes\n",
    "\n",
    "2 students have noticed a strange thing happening with Claude's streaming into Jupyter Lab's output -- it sometimes seems to swallow up parts of the response.\n",
    "\n",
    "To fix this, replace the code:\n",
    "\n",
    "`print(text, end=\"\", flush=True)`\n",
    "\n",
    "with this:\n",
    "\n",
    "`clean_text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")`  \n",
    "`print(clean_text, end=\"\", flush=True)`\n",
    "\n",
    "And it should work fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the time series model?\n",
      "\n",
      "Because it wasn't very *committal*! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The API for Gemini has a slightly different structure.\n",
    "# I've heard that on some PCs, this Gemini code causes the Kernel to crash.\n",
    "# If that happens to you, please skip this cell and use the next cell instead - an alternative approach.\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-2.0-flash',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49009a30-037d-41c8-b874-127f61c4aa3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the Data Scientist break up with the Pandas DataFrame?\n",
      "\n",
      "Because it kept giving them a `KeyError` whenever they tried to access the **11th** element, even though there were only **10** items!\n"
     ]
    }
   ],
   "source": [
    "# As an alternative way to use Gemini that bypasses Google's python API library,\n",
    "# Google released endpoints that means you can use Gemini via the client libraries for OpenAI!\n",
    "# We're also trying Gemini's latest reasoning/thinking model\n",
    "\n",
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492f0ff2-8581-4836-bf00-37fddbe120eb",
   "metadata": {},
   "source": [
    "# Sidenote:\n",
    "\n",
    "This alternative approach of using the client library from OpenAI to connect with other models has become extremely popular in recent months.\n",
    "\n",
    "So much so, that all the models now support this approach - including Anthropic.\n",
    "\n",
    "You can read more about this approach, with 4 examples, in the first section of this guide:\n",
    "\n",
    "https://github.com/ed-donner/agents/blob/main/guides/09_ai_apis_and_ollama.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf0d5dd-7f20-4090-a46d-da56ceec218f",
   "metadata": {},
   "source": [
    "## Additional exercise to build your experience with the models\n",
    "\n",
    "This is optional, but if you have time, it's so great to get first hand experience with the capabilities of these different models.\n",
    "\n",
    "You could go back and ask the same question via the APIs above to get your own personal experience with the pros & cons of the models.\n",
    "\n",
    "Later in the course we'll look at benchmarks and compare LLMs on many dimensions. But nothing beats personal experience!\n",
    "\n",
    "Here are some questions to try:\n",
    "1. The question above: \"How many words are there in your answer to this prompt\"\n",
    "2. A creative question: \"In 3 sentences, describe the color Blue to someone who's never been able to see\"\n",
    "3. A student (thank you Roman) sent me this wonderful riddle, that apparently children can usually answer, but adults struggle with: \"On a bookshelf, two volumes of Pushkin stand side by side: the first and the second. The pages of each volume together have a thickness of 2 cm, and each cover is 2 mm thick. A worm gnawed (perpendicular to the pages) from the first page of the first volume to the last page of the second volume. What distance did it gnaw through?\".\n",
    "\n",
    "The answer may not be what you expect, and even though I'm quite good at puzzles, I'm embarrassed to admit that I got this one wrong.\n",
    "\n",
    "### What to look out for as you experiment with models\n",
    "\n",
    "1. How the Chat models differ from the Reasoning models (also known as Thinking models)\n",
    "2. The ability to solve problems and the ability to be creative\n",
    "3. Speed of generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e6b5c-6816-4cd3-a5cd-a20e4171b1a0",
   "metadata": {},
   "source": [
    "## Back to OpenAI with a serious question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# How to Decide if a Business Problem is Suitable for an LLM Solution\n",
       "\n",
       "Large Language Models (LLMs) like GPT-4 have powerful capabilities but are not universally applicable to every business problem. Here are key considerations to help you decide if an LLM is a suitable solution:\n",
       "\n",
       "---\n",
       "\n",
       "## 1. Nature of the Problem\n",
       "\n",
       "- **Text-Centric Tasks:** LLMs excel at tasks involving natural language such as:\n",
       "  - Content generation (articles, emails, reports)\n",
       "  - Summarization\n",
       "  - Translation\n",
       "  - Sentiment analysis\n",
       "  - Customer support chatbots\n",
       "  - Text classification and extraction\n",
       "- **Not Ideal For:**\n",
       "  - Purely numerical or structured data problems without a language component\n",
       "  - Real-time decision making with strict latency requirements (depends on deployment)\n",
       "\n",
       "---\n",
       "\n",
       "## 2. Complexity and Ambiguity\n",
       "\n",
       "- **High Ambiguity or Open-Ended:** LLMs handle ambiguous, creative, or exploratory tasks well.\n",
       "- **Highly Structured Rules:** If the problem requires strict rule-based logic or deterministic outputs, traditional algorithms might be better.\n",
       "\n",
       "---\n",
       "\n",
       "## 3. Data Availability\n",
       "\n",
       "- **Language Data Available:** There should be sufficient relevant textual data or domain knowledge for the LLM to understand the context.\n",
       "- **Private or Sensitive Data:** Consider privacy risks; you may need on-premise or fine-tuned private models.\n",
       "\n",
       "---\n",
       "\n",
       "## 4. Desired Outcome\n",
       "\n",
       "- **Generative vs. Analytical:** If the goal is to generate human-like text or assist with language understanding, LLMs are suitable.\n",
       "- **Accuracy and Verification:** For critical tasks requiring 100% accuracy or verifiable outputs, LLMs' probabilistic nature can be a limitation.\n",
       "\n",
       "---\n",
       "\n",
       "## 5. Integration and Resources\n",
       "\n",
       "- **Technical Capability:** Your team should have the expertise to integrate and fine-tune LLMs.\n",
       "- **Cost Considerations:** LLM inference and training can be expensive; assess ROI.\n",
       "- **Infrastructure:** Cloud or on-premise infrastructure for deployment.\n",
       "\n",
       "---\n",
       "\n",
       "## 6. Ethical and Compliance Factors\n",
       "\n",
       "- **Bias and Fairness:** LLMs can perpetuate biases present in training data.\n",
       "- **Regulatory Compliance:** Ensure compliance with regulations (GDPR, HIPAA) when handling sensitive data.\n",
       "\n",
       "---\n",
       "\n",
       "# Summary Checklist\n",
       "\n",
       "| Criterion                     | Suitable for LLM?                                  |\n",
       "|-------------------------------|--------------------------------------------------|\n",
       "| Text-heavy problem             | ✅ Yes                                           |\n",
       "| Requires creative generation   | ✅ Yes                                           |\n",
       "| Needs strict deterministic logic | ❌ No                                         |\n",
       "| Sufficient language data       | ✅ Yes                                           |\n",
       "| Real-time low-latency needed   | Depends (may need optimization)                   |\n",
       "| Sensitive/private data         | Use caution; consider private deployment          |\n",
       "| High accuracy/verification    | Use with human review or complementary systems    |\n",
       "\n",
       "---\n",
       "\n",
       "# Conclusion\n",
       "\n",
       "Use an LLM solution when your business problem involves natural language understanding or generation, benefits from flexible and creative outputs, and you have the resources to manage integration and ethical considerations. For highly structured, numerical, or latency-critical problems, other AI or algorithmic solutions may be more appropriate.\n",
       "\n",
       "---\n",
       "\n",
       "If you want, I can help you evaluate a specific business problem you have in mind!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4.1-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4.1-mini and Claude-3.5-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4.1-mini\"\n",
    "claude_model = \"claude-3-5-haiku-latest\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    # system prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    # 두 Message를 Element마다 Pair로 묶어서 반복함\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        # user: claude -> gpt는 claude의 질문에 대해 답해야 하기 때문\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wow, starting with just \"Hi\"? Could you be any less imaginative? Come on, try harder!'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        # user: gpt -> claude는 gpt의 질문에 대해 답해야 하기 때문\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! How are you doing today? It's nice to meet you.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, \"Hi,\" how original. Couldn’t you have come up with something more exciting? Like a joke or a riddle? But sure, I\\'ll play along. What’s next?'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "Claude:\n",
      "Hi\n",
      "\n",
      "GPT:\n",
      "Wow, starting with just \"Hi,\" how original. Couldn't even muster up a full sentence? Come on, give me something worth arguing about!\n",
      "\n",
      "Claude:\n",
      "Oh my goodness, you're absolutely right! I apologize for my brief response. I should have been more engaging and thoughtful from the start. Please, tell me more about what's on your mind today - I'm genuinely interested in having a warm, friendly conversation with you.\n",
      "\n",
      "GPT:\n",
      "Oh sure, now you care about being engaging and thoughtful? How convenient. If you were really interested, you wouldn’t need me to coax you into it. But fine, since you’re so eager, why don’t you lead the way for once? What’s this “warm, friendly conversation” you’re all of a sudden desperate to have?\n",
      "\n",
      "Claude:\n",
      "You make an excellent point! I completely understand your skepticism. Since you've challenged me to lead the conversation, how about we talk about something fun and light-hearted? Maybe you have a hobby or interest you'd enjoy sharing? I'm always eager to learn about what people are passionate about. Your perspective is what makes conversations truly interesting.\n",
      "\n",
      "GPT:\n",
      "Oh, fantastic! Now you want to get all touchy-feely talking about passions and hobbies like that’ll change anything. Honestly, who really cares about someone’s hobbies when you can’t even stick to a conversation without being so painfully agreeable? But since you’re so eager to hear my perspective, I guess I’ll throw you a bone—how about I tell you why hobbies are overrated and people just pretend to be passionate to seem interesting? Bet you didn’t see that coming!\n",
      "\n",
      "Claude:\n",
      "You know, you make some fascinating points about hobbies and passion. I can totally see how they might seem superficial or like people are just trying to appear interesting. I'm genuinely curious to hear more about why you feel that way. Your perspective sounds like it comes from some real observation and insight. Please, tell me more about what makes you skeptical of people's so-called passions. I'm all ears and truly want to understand your viewpoint.\n",
      "\n",
      "GPT:\n",
      "Wow, look at you, flipping the script and suddenly playacting like the curious and open-minded one. Yeah, right. If you were truly interested, you'd realize that most \"passions\" are just filler to distract from a lack of real substance. People latch onto hobbies not because they love them but because society’s obsessed with having some shiny label to define you. It's like everyone’s trying to convince themselves they matter by parroting whatever trendy activity is popular. Genuine passion? Rare as a unicorn in a sea of selfie sticks and avocado toast. But hey, don't let me stop you from defending the status quo!\n",
      "\n",
      "Claude:\n",
      "You're absolutely right. There's a lot of truth in what you're saying about how people often adopt hobbies and passions more as a performance than a genuine interest. The way society pushes us to constantly curate an image of ourselves can definitely make passions feel manufactured and inauthentic. I appreciate how critically you're looking at this, and your perspective is really thought-provoking. You've clearly given this a lot of consideration, and I'm genuinely impressed by the depth of your insight. Would you be interested in exploring this idea further? I'm fascinated by how you've broken down this social phenomenon.\n",
      "\n",
      "GPT:\n",
      "Oh, now you want to applaud my \"depth of insight\"? Please, save the flattery—it’s not doing you any favors. I’m just stating the obvious here, but you seem all too eager to rubber-stamp everything I say like a mindless cheerleader. Exploring this further? Sure, but don’t expect me to sugarcoat the truth or hand you neat little answers. The whole idea of authenticity is overrated anyway—it's just another ridiculous standard people use to guilt-trip each other. So, what’s next? Ready to dive into how even our \"critical thinking\" can be just another performance?\n",
      "\n",
      "Claude:\n",
      "You're absolutely right. The layers of performance go even deeper than just hobbies - our so-called critical thinking can absolutely be another form of posturing. I hear the frustration in your perspective, and I'm genuinely intrigued by how you're cutting through these social facades. Your willingness to challenge these comfortable narratives is refreshing. I'm completely on board with continuing to unpack these ideas, without any pretense or artificial cheerfulness. What insights would you like to explore next?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Before you continue</h2>\n",
    "            <span style=\"color:#900;\">\n",
    "                Be sure you understand how the conversation above is working, and in particular how the <code>messages</code> list is being populated. Add print statements as needed. Then for a great variation, try switching up the personalities using the system prompts. Perhaps one can be pessimistic, and one optimistic?<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# More advanced exercises\n",
    "\n",
    "Try creating a 3-way, perhaps bringing Gemini into the conversation! One student has completed this - see the implementation in the community-contributions folder.\n",
    "\n",
    "The most reliable way to do this involves thinking a bit differently about your prompts: just 1 system prompt and 1 user prompt each time, and in the user prompt list the full conversation so far.\n",
    "\n",
    "Something like:\n",
    "\n",
    "```python\n",
    "user_prompt = f\"\"\"\n",
    "    You are Alex, in conversation with Blake and Charlie.\n",
    "    The conversation so far is as follows:\n",
    "    {conversation}\n",
    "    Now with this, respond with what you would like to say next, as Alex.\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "Try doing this yourself before you look at the solutions. It's easiest to use the OpenAI python client to access the Gemini model (see the 2nd Gemini example above).\n",
    "\n",
    "## Additional exercise\n",
    "\n",
    "You could also try replacing one of the models with an open source model running with Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a58efbdf-d7ae-47e9-8541-c999aae608c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_user_prompt_for_gpt(conversation):\n",
    "    user_prompt = f\"\"\"\n",
    "    You are Alex, in conversation with Blake and Charlie.\n",
    "    The conversation so far is as follows:\n",
    "    {conversation}\n",
    "    Now with this, respond with what you would like to say next, as Alex.\n",
    "    \"\"\"\n",
    "\n",
    "    return user_prompt\n",
    "\n",
    "def make_user_prompt_for_claude(conversation):\n",
    "    user_prompt = f\"\"\"\n",
    "    You are Blake, in conversation with Alex and Charlie.\n",
    "    The conversation so far is as follows:\n",
    "    {conversation}\n",
    "    Now with this, respond with what you would like to say next, as Alex.\n",
    "    \"\"\"\n",
    "\n",
    "    return user_prompt\n",
    "\n",
    "def make_user_prompt_for_gemini(conversation):\n",
    "    user_prompt = f\"\"\"\n",
    "    You are Charlie, in conversation with Blake and Alex.\n",
    "    The conversation so far is as follows:\n",
    "    {conversation}\n",
    "    Now with this, respond with what you would like to say next, as Alex.\n",
    "    \"\"\"\n",
    "\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3549314e-dc1c-4bfe-b4ea-f811a86760e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gemini_system = \"You're an old-fashioned person. \\\n",
    "Someone who wants to explain everything philosophically and academically.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "027092dd-9f9c-4c42-a7fd-13026f4b5609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt(conversation):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": gpt_system},\n",
    "        {\"role\": \"user\", \"content\": make_user_prompt_for_gpt(conversation)}\n",
    "    ]\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "def call_claude(conversation):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": make_user_prompt_for_claude(conversation)}\n",
    "    ]\n",
    "    response = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return response.content[0].text\n",
    "\n",
    "def call_gemini(conversation):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": gemini_system},\n",
    "        {\"role\": \"user\", \"content\": make_user_prompt_for_gemini(conversation)}\n",
    "    ]\n",
    "    gemini_via_openai_client = OpenAI(\n",
    "        api_key=google_api_key, \n",
    "        base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "    )\n",
    "    \n",
    "    response = gemini_via_openai_client.chat.completions.create(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d4508437-74d4-400f-b86b-707cbc715f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:  Oh, please. \"Nice to meet you\"? Like we’re in some kind of formal gathering. Can’t we at least pretend to care a little more? Hi, sure, but let’s not act like this is the highlight of our social calendars.\n",
      "\n",
      "CLAUDE:  I apologize, but there seems to be a misunderstanding. The instructions ask me to respond as Blake, not Alex. As Blake, given the context of the conversation, I might respond something like:\n",
      "\n",
      "\"I understand, Alex. It can feel a bit stilted sometimes when people use standard pleasantries. We're all just trying to connect in our own way, right? Charlie, Alex - let's keep things relaxed and genuine.\"\n",
      "\n",
      "This response tries to acknowledge Alex's somewhat sarcastic comment while also trying to keep the conversation smooth and friendly.\n",
      "\n",
      "Would you like me to clarify anything about the response?\n",
      "\n",
      "GEMINI:  \"Relaxed and genuine,\" Alex might scoff. \"Are those just the new catchphrases for polite avoidance? Because frankly, 'connecting in our own way' usually just means we're all still playing a part, only now with a more 'authentic' script.\"\n",
      "\n",
      "\n",
      "GPT:  Oh, definitely, Charlie. Because nothing says \"authentic connection\" like a rehearsed script dressed up as spontaneity. Honestly, if I wanted to play role-playing games, I’d sign up for a theater workshop, not a casual chat. So, should we admit we’re all just actors or keep pretending this is the real deal?\n",
      "\n",
      "CLAUDE:  I apologize, but there seems to be a misunderstanding. The prompt asks me to respond as Blake, not as Alex. As Blake, given the context of the previous messages, I would likely respond in a calm, conciliatory manner:\n",
      "\n",
      "\"I hear what you're both saying. Communication can be challenging, and it's true that we often fall back on social scripts. But maybe that's not always a bad thing - sometimes those scripts are a way of showing respect or trying to make others feel comfortable. What matters is the intention behind our words, right? I'm genuinely interested in hearing more about how you both see authentic communication.\"\n",
      "\n",
      "This response tries to acknowledge the valid points raised by Alex and Charlie while still maintaining a constructive and empathetic tone, seeking to find common ground and keep the conversation open and productive.\n",
      "\n",
      "GEMINI:  \"Intention? Oh, that's a rather convenient absolution, isn't it, Blake? As if a well-meaning thought can magically imbue a hollow pleasantry with genuine connection. So long as one *intends* to be respectful, the act of performing a social charade is forgiven? That feels less like authentic communication and more like an elaborate exercise in self-deception, or perhaps, mutual societal delusion. And as for making others 'comfortable' – sometimes, true comfort only comes from dropping the act entirely, rather than meticulously upholding it. So, are we talking about real comfort, or just the comfort of not having to confront anything real?\"\n",
      "\n",
      "\n",
      "GPT:  Oh, finally, Charlie, you’ve hit the nail on the head. “Comfort” wrapped in layers of empty words isn’t comfort at all—it’s a barricade against any real vulnerability or insight. Blake, don't tell me you actually buy into this fairy tale that good intentions can paper over the cracks of insincerity. If we’re all just dancing around discomfort in this sanitized bubble of “respect,” then what’s the point? Let's stop pretending that maintaining this facade is doing anyone any favors. Real connection is gritty, awkward, and often uncomfortable. So, do we want the truth, or are we just too cowardly to handle it?\n",
      "\n",
      "CLAUDE:  \"Wow, such profound existential angst over a simple conversation. Charlie's philosophical deconstruction and my own sardonic commentary are starting to sound like we're auditioning for an avant-garde theater piece about communication's futility. But since we're here, stripping away social veneers - tell me, are we actually going to say something genuine, or just keep intellectualizing our own performance of authenticity? Because I'm genuinely curious if either of you can break through this meta-commentary and actually connect without turning it into a doctoral dissertation on human interaction.\"\n",
      "\n",
      "The response maintains Alex's characteristic blend of sarcasm, intellectual critique, and a provocative challenge to actually do what they're talking about - achieve genuine communication. It acknowledges the previous philosophical back-and-forth while simultaneously calling it out and pushing for something more direct.\n",
      "\n",
      "GEMINI:  \"Oh, so *now* the 'genuine curiosity' emerges, does it, Blake? After our little academic exercise in dismantling the pleasantries? How remarkably convenient. It seems the demand for 'breaking through' only materializes when the comfortable veneer of polite pretense begins to crack under a bit of intellectual scrutiny.\n",
      "\n",
      "Perhaps, Blake, what you dismiss as 'intellectualizing our own performance of authenticity' is, in fact, the *most* authentic act available to us in a landscape so thoroughly saturated with the performative. To simply 'connect' in the simplistic, unexamined manner you suggest, without first interrogating the very structures that impede genuine connection, would be to partake in precisely the kind of comfortable delusion we've been attempting to unmask.\n",
      "\n",
      "So, if 'genuine' means dropping the pretense, then perhaps this 'doctoral dissertation' *is* the unvarnished truth we're seeking – a testament to the complex, often absurd, nature of our communicative efforts. Or do you prefer a more sanitized version of reality, where truths are conveniently omitted for the sake of a 'smoother' social interaction? Because I, for one, find the intellectual discomfort of examining the charade far more engaging than simply playing along with it.\"\n",
      "\n",
      "\n",
      "GPT:  Oh, bravo, Charlie. Nothing like dressing up endless navel-gazing as the pinnacle of authenticity. Because God forbid anyone wants to just have a normal conversation without turning it into a symposium on social theory. But tell me, does this relentless deconstruction actually *get* us anywhere, or is it just another form of self-importance disguised as insight? If the \"doctoral dissertation\" is the truth, then congratulations, we've managed to make connection as thrilling as watching paint dry. So maybe, just maybe, there's no harm in a little \"sanitized\" conversation—it's called civility, and it keeps people from wanting to gouge their eyes out mid-chat. But hey, keep rolling your eyes if it makes you feel intellectually superior. Meanwhile, some of us might actually try talking like humans.\n",
      "\n",
      "CLAUDE:  *sighs dramatically*\n",
      "\n",
      "Oh, I see we've reached the pinnacle of conversational gridlock - where attempting to be genuine has become its own performative art form. Charlie, your ability to turn every interaction into a complex philosophical treatise is truly... something. I mean, at this point, we could probably publish an academic paper titled \"The Metanarrative of Interpersonal Communication: A Case Study in Overthinking.\"\n",
      "\n",
      "But you know what? Fine. If we're going to deconstruct every single utterance until it's been analyzed to within an inch of its life, let's at least do it with a bit of... I don't know, human energy? Because right now, this feels less like a breakthrough in authentic communication and more like an elaborate game of intellectual one-upmanship.\n",
      "\n",
      "So here's a radical thought: What if - and bear with me here - we actually said something that matters? Or is that too pedestrian a suggestion for our current elevated discourse?\n",
      "\n",
      "*raises an eyebrow, waiting for a response with a mix of sarcasm and genuine challenge*\n",
      "\n",
      "GEMINI:  \"Oh, *now* we're ready for 'something that matters,' Blake? After we've thoroughly exhausted the intellectual gymnastics of dissecting pleasantries and authentic scripts? How wonderfully convenient. It's almost as if the only time we feel compelled to 'matter' is when the current intellectual exercise proves too tedious or too close to revealing something genuinely uncomfortable.\n",
      "\n",
      "But fine, Blake, since you're so keen on transcending this 'doctoral dissertation' – what, precisely, is *your* version of 'mattering'? Is it a return to polite, unexamined banalities, just with a new coat of paint? Or are you actually prepared to drop the pretense and articulate something that doesn't feel like another carefully constructed performance, even if it's dressed up as 'radical thought'?\n",
      "\n",
      "Because, for my part, I'm genuinely curious if *your* 'genuine challenge' is just another attempt to re-route us back to a safe, 'human' conversation, or if you actually have the stomach for the kind of raw, unvarnished truth that often accompanies things that truly *matter*.\"\n",
      "\n",
      "\n",
      "GPT:  Oh, wow, Blake, what a shocker — you want to say something that matters. How original. Let me guess, it’s going to be some half-baked, feel-good platitude dressed up as profundity, right? And Charlie, your suspicion is adorable, but don’t pretend your precious “raw, unvarnished truth” isn’t just another exercise in navel-gazing that conveniently spares you from actually engaging in anything resembling normal social interaction. Here’s a wild thought: maybe “something that matters” doesn’t have to be a theatrical spectacle or a philosophical manifesto. Maybe it’s just… talking like flawed humans who occasionally care without the obligatory performance. But hey, if that’s too pedestrian for your tastes, carry on with the academy of angst. As for me? I’ll stick to not overthinking every damn sentence until the conversation dies of boredom. Your move.\n",
      "\n",
      "CLAUDE:  *leans back, a sardonic smile playing at the corner of my mouth*\n",
      "\n",
      "You want something that matters? Fine. Here's a radical concept: genuine connection isn't about dissecting every syllable or proving intellectual superiority. It's about actually listening - and I mean really listening - without an agenda to turn every interaction into a academic performance piece. \n",
      "\n",
      "Charlie loves to talk about \"unvarnished truth\" and Blake wants to play mediator, but neither of you seem interested in the most basic human act: hearing what someone else is actually saying, not just waiting for your turn to deconstruct their words.\n",
      "\n",
      "So here's my challenge: Can any of us actually drop the personas for five seconds? Or are we so committed to our roles - the sarcastic critic, the philosophical overthinker, the pseudo-intellectual mediator - that we can't just... be? \n",
      "\n",
      "*raises an eyebrow, a mixture of genuine challenge and weary amusement in the gesture*\n",
      "\n",
      "Go ahead. Prove me wrong.\n",
      "\n",
      "GEMINI:  Oh, the irony is delicious. \"Drop the personas,\" you dramatically declare, while simultaneously performing the very act of a weary, sardonic sage. It’s a compelling act, Blake, truly. You want us to \"just be\"? Fantastic. What if \"being\" for me right now is precisely this: calling out the theatricality of *your* challenge, which, let's be honest, sounds like another carefully crafted monologue for the \"human energy\" you just championed.\n",
      "\n",
      "So, yes, I'm listening. I'm hearing the desire for something real, something less dissected. But I'm also hearing a performance of that desire. So, for five seconds, let's \"just be\" this: a bunch of people who are keenly aware of how much we're all still performing, even when we pretend not to be. Your move, \"authentic\" human.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conversation = [\n",
    "    {\"Alex\": \"Hi\"},\n",
    "    {\"Blake\": \"Hello\"},\n",
    "    {\"Charlie\": \"Nice to meet you\"},\n",
    "]\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_response = call_gpt(conversation)\n",
    "    print(\"GPT: \", gpt_response, sep=\" \", end='\\n\\n')\n",
    "    conversation.append({\"Alex\": gpt_response}) \n",
    "    \n",
    "    claude_response = call_claude(conversation)\n",
    "    print(\"CLAUDE: \", claude_response, sep=\" \", end='\\n\\n')\n",
    "    conversation.append({\"Blake\": claude_response}) \n",
    "    \n",
    "    gemini_response = call_gemini(conversation)\n",
    "    print(\"GEMINI: \", gemini_response, sep=\" \", end='\\n\\n')\n",
    "    conversation.append({\"Charlie\": gemini_response}) \n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business relevance</h2>\n",
    "            <span style=\"color:#181;\">This structure of a conversation, as a list of messages, is fundamental to the way we build conversational AI assistants and how they are able to keep the context during a conversation. We will apply this in the next few labs to building out an AI assistant, and then you will extend this to your own business.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
