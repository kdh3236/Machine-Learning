{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75e2ef28-594f-4c18-9d22-c6b8cd40ead2",
   "metadata": {},
   "source": [
    "# Day 3 - Conversational AI - aka Chatbot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70e39cd8-ec79-4e3e-9c26-5659d42d0861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "231605aa-fccb-447e-89cf-8b187444536a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AIzaSyAX\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6541d58e-2297-4de1-b1f7-77da1b98b8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "\n",
    "openai = OpenAI()\n",
    "MODEL = 'gpt-4o-mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e16839b5-c03b-4d9d-add6-87a0f6f37575",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e97227-f162-4d1a-a0b2-345ff248cbe7",
   "metadata": {},
   "source": [
    "# Please read this! A change from the video:\n",
    "\n",
    "In the video, I explain how we now need to write a function called:\n",
    "\n",
    "`chat(message, history)`\n",
    "\n",
    "Which expects to receive `history` in a particular format, which we need to map to the OpenAI format before we call OpenAI:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "But Gradio has been upgraded! Now it will pass in `history` in the exact OpenAI format, perfect for us to send straight to OpenAI.\n",
    "\n",
    "So our work just got easier!\n",
    "\n",
    "We will write a function `chat(message, history)` where:  \n",
    "**message** is the prompt to use  \n",
    "**history** is the past conversation, in OpenAI format  \n",
    "\n",
    "We will combine the system message, history and latest message, then call OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1eacc8a4-4b48-4358-9e06-ce0020041bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpler than in my video - we can easily create this function that calls OpenAI\n",
    "# It's now just 1 line of code to prepare the input to OpenAI!\n",
    "\n",
    "# Student Octavio O. has pointed out that this isn't quite as straightforward for Claude -\n",
    "# see the excellent contribution in community-contributions \"Gradio_issue_with_Claude\" that handles Claude.\n",
    "\n",
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    print(\"History is:\")\n",
    "    print(history)\n",
    "    print(\"And messages is:\")\n",
    "    print(messages)\n",
    "\n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1334422a-808f-4147-9c4c-57d63d9780d0",
   "metadata": {},
   "source": [
    "## And then enter Gradio's magic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0866ca56-100a-44ab-8bd0-1568feaf6bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7871\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7871/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History is:\n",
      "[]\n",
      "And messages is:\n",
      "[{'role': 'system', 'content': 'You are a helpful assistant'}, {'role': 'user', 'content': 'Hello'}]\n",
      "History is:\n",
      "[{'role': 'user', 'metadata': None, 'content': 'Hello', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'Hello! How can I assist you today?', 'options': None}]\n",
      "And messages is:\n",
      "[{'role': 'system', 'content': 'You are a helpful assistant'}, {'role': 'user', 'metadata': None, 'content': 'Hello', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'Hello! How can I assist you today?', 'options': None}, {'role': 'user', 'content': 'It is just test about gradio'}]\n",
      "History is:\n",
      "[{'role': 'user', 'metadata': None, 'content': 'Hello', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'Hello! How can I assist you today?', 'options': None}, {'role': 'user', 'metadata': None, 'content': 'It is just test about gradio', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': \"Got it! If you have any questions about Gradio or if you'd like to test something specific, feel free to ask. I'm here to help!\", 'options': None}]\n",
      "And messages is:\n",
      "[{'role': 'system', 'content': 'You are a helpful assistant'}, {'role': 'user', 'metadata': None, 'content': 'Hello', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'Hello! How can I assist you today?', 'options': None}, {'role': 'user', 'metadata': None, 'content': 'It is just test about gradio', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': \"Got it! If you have any questions about Gradio or if you'd like to test something specific, feel free to ask. I'm here to help!\", 'options': None}, {'role': 'user', 'content': 'I don\\'t understand how the message of OpenAI package\\'s message can be formed as messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]'}]\n",
      "History is:\n",
      "[{'role': 'user', 'metadata': None, 'content': 'Hello', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'Hello! How can I assist you today?', 'options': None}, {'role': 'user', 'metadata': None, 'content': 'It is just test about gradio', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': \"Got it! If you have any questions about Gradio or if you'd like to test something specific, feel free to ask. I'm here to help!\", 'options': None}, {'role': 'user', 'metadata': None, 'content': 'I don\\'t understand how the message of OpenAI package\\'s message can be formed as messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'In the context of using the OpenAI API, particularly for conversational models like ChatGPT, the message structure you mentioned is designed to provide context for the conversation. Let\\'s break down the components of the message array you mentioned:\\n\\n1. **System Message**: This is an initial message that sets the context or behavior of the assistant. It generally provides instructions about how the assistant should behave or what information it should consider while replying.\\n\\n   ```python\\n   {\"role\": \"system\", \"content\": system_message}\\n   ```\\n\\n2. **History**: This refers to the conversation history which usually consists of previous messages exchanged between the user and the assistant. Each message in the history follows a similar format, indicating whether the message was from the user or the assistant.\\n\\n   ```python\\n   history  # This is a list of previous messages in the same format.\\n   ```\\n\\n3. **User Message**: This is the current message from the user. It allows the model to understand what the user is asking or saying at this moment.\\n\\n   ```python\\n   {\"role\": \"user\", \"content\": message}\\n   ```\\n\\n### Combining Them\\n\\nWhen you combine these three components, the resulting list of messages will form a complete input that represents the entire conversation context for the model:\\n\\n```python\\nmessages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\\n```\\n\\n- **`[{\"role\": \"system\", \"content\": system_message}]`**: This creates a list with one item, the system message.\\n- **`+ history`**: This concatenates the existing history of messages to the list.\\n- **`+ [{\"role\": \"user\", \"content\": message}]`**: This adds the current user message as the last item in the list.\\n\\n### Example\\n\\nLet\\'s illustrate this with an example:\\n\\n```python\\nsystem_message = \"You are a helpful assistant.\"\\nhistory = [\\n    {\"role\": \"user\", \"content\": \"Hello!\"},\\n    {\"role\": \"assistant\", \"content\": \"Hi there! How can I help you today?\"}\\n]\\nmessage = \"Can you tell me about Python?\"\\n\\nmessages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\\n```\\n\\nThe `messages` variable would now look like this:\\n\\n```python\\n[\\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\\n    {\"role\": \"user\", \"content\": \"Hello!\"},\\n    {\"role\": \"assistant\", \"content\": \"Hi there! How can I help you today?\"},\\n    {\"role\": \"user\", \"content\": \"Can you tell me about Python?\"}\\n]\\n```\\n\\nThis structure allows the model to have all relevant context about the conversation so it can generate an appropriate response.\\n\\nIf you have more questions or need clarification, feel free to ask!', 'options': None}]\n",
      "And messages is:\n",
      "[{'role': 'system', 'content': 'You are a helpful assistant'}, {'role': 'user', 'metadata': None, 'content': 'Hello', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'Hello! How can I assist you today?', 'options': None}, {'role': 'user', 'metadata': None, 'content': 'It is just test about gradio', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': \"Got it! If you have any questions about Gradio or if you'd like to test something specific, feel free to ask. I'm here to help!\", 'options': None}, {'role': 'user', 'metadata': None, 'content': 'I don\\'t understand how the message of OpenAI package\\'s message can be formed as messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'In the context of using the OpenAI API, particularly for conversational models like ChatGPT, the message structure you mentioned is designed to provide context for the conversation. Let\\'s break down the components of the message array you mentioned:\\n\\n1. **System Message**: This is an initial message that sets the context or behavior of the assistant. It generally provides instructions about how the assistant should behave or what information it should consider while replying.\\n\\n   ```python\\n   {\"role\": \"system\", \"content\": system_message}\\n   ```\\n\\n2. **History**: This refers to the conversation history which usually consists of previous messages exchanged between the user and the assistant. Each message in the history follows a similar format, indicating whether the message was from the user or the assistant.\\n\\n   ```python\\n   history  # This is a list of previous messages in the same format.\\n   ```\\n\\n3. **User Message**: This is the current message from the user. It allows the model to understand what the user is asking or saying at this moment.\\n\\n   ```python\\n   {\"role\": \"user\", \"content\": message}\\n   ```\\n\\n### Combining Them\\n\\nWhen you combine these three components, the resulting list of messages will form a complete input that represents the entire conversation context for the model:\\n\\n```python\\nmessages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\\n```\\n\\n- **`[{\"role\": \"system\", \"content\": system_message}]`**: This creates a list with one item, the system message.\\n- **`+ history`**: This concatenates the existing history of messages to the list.\\n- **`+ [{\"role\": \"user\", \"content\": message}]`**: This adds the current user message as the last item in the list.\\n\\n### Example\\n\\nLet\\'s illustrate this with an example:\\n\\n```python\\nsystem_message = \"You are a helpful assistant.\"\\nhistory = [\\n    {\"role\": \"user\", \"content\": \"Hello!\"},\\n    {\"role\": \"assistant\", \"content\": \"Hi there! How can I help you today?\"}\\n]\\nmessage = \"Can you tell me about Python?\"\\n\\nmessages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\\n```\\n\\nThe `messages` variable would now look like this:\\n\\n```python\\n[\\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\\n    {\"role\": \"user\", \"content\": \"Hello!\"},\\n    {\"role\": \"assistant\", \"content\": \"Hi there! How can I help you today?\"},\\n    {\"role\": \"user\", \"content\": \"Can you tell me about Python?\"}\\n]\\n```\\n\\nThis structure allows the model to have all relevant context about the conversation so it can generate an appropriate response.\\n\\nIf you have more questions or need clarification, feel free to ask!', 'options': None}, {'role': 'user', 'content': 'Can you translate it to Korean?'}]\n",
      "History is:\n",
      "[{'role': 'user', 'metadata': None, 'content': 'Hello', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'Hello! How can I assist you today?', 'options': None}, {'role': 'user', 'metadata': None, 'content': 'It is just test about gradio', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': \"Got it! If you have any questions about Gradio or if you'd like to test something specific, feel free to ask. I'm here to help!\", 'options': None}, {'role': 'user', 'metadata': None, 'content': 'I don\\'t understand how the message of OpenAI package\\'s message can be formed as messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'In the context of using the OpenAI API, particularly for conversational models like ChatGPT, the message structure you mentioned is designed to provide context for the conversation. Let\\'s break down the components of the message array you mentioned:\\n\\n1. **System Message**: This is an initial message that sets the context or behavior of the assistant. It generally provides instructions about how the assistant should behave or what information it should consider while replying.\\n\\n   ```python\\n   {\"role\": \"system\", \"content\": system_message}\\n   ```\\n\\n2. **History**: This refers to the conversation history which usually consists of previous messages exchanged between the user and the assistant. Each message in the history follows a similar format, indicating whether the message was from the user or the assistant.\\n\\n   ```python\\n   history  # This is a list of previous messages in the same format.\\n   ```\\n\\n3. **User Message**: This is the current message from the user. It allows the model to understand what the user is asking or saying at this moment.\\n\\n   ```python\\n   {\"role\": \"user\", \"content\": message}\\n   ```\\n\\n### Combining Them\\n\\nWhen you combine these three components, the resulting list of messages will form a complete input that represents the entire conversation context for the model:\\n\\n```python\\nmessages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\\n```\\n\\n- **`[{\"role\": \"system\", \"content\": system_message}]`**: This creates a list with one item, the system message.\\n- **`+ history`**: This concatenates the existing history of messages to the list.\\n- **`+ [{\"role\": \"user\", \"content\": message}]`**: This adds the current user message as the last item in the list.\\n\\n### Example\\n\\nLet\\'s illustrate this with an example:\\n\\n```python\\nsystem_message = \"You are a helpful assistant.\"\\nhistory = [\\n    {\"role\": \"user\", \"content\": \"Hello!\"},\\n    {\"role\": \"assistant\", \"content\": \"Hi there! How can I help you today?\"}\\n]\\nmessage = \"Can you tell me about Python?\"\\n\\nmessages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\\n```\\n\\nThe `messages` variable would now look like this:\\n\\n```python\\n[\\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\\n    {\"role\": \"user\", \"content\": \"Hello!\"},\\n    {\"role\": \"assistant\", \"content\": \"Hi there! How can I help you today?\"},\\n    {\"role\": \"user\", \"content\": \"Can you tell me about Python?\"}\\n]\\n```\\n\\nThis structure allows the model to have all relevant context about the conversation so it can generate an appropriate response.\\n\\nIf you have more questions or need clarification, feel free to ask!', 'options': None}, {'role': 'user', 'metadata': None, 'content': 'Can you translate it to Korean?', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': '물론입니다! 위에서 설명한 내용을 한국어로 번역해 드리겠습니다.\\n\\n---\\n\\nOpenAI API를 사용할 때, 특히 ChatGPT와 같은 대화형 모델에 대해 말씀하신 메시지 구조는 대화의 맥락을 제공하기 위해 설계되었습니다. 구성 요소를 나눠서 살펴보겠습니다:\\n\\n1. **시스템 메시지**: 이는 어시스턴트의 맥락이나 행동을 설정하는 초기 메시지입니다. 일반적으로 어시스턴트가 어떻게 행동해야 하는지 또는 어떤 정보를 고려해야 하는지에 대한 지침을 제공합니다.\\n\\n   ```python\\n   {\"role\": \"system\", \"content\": system_message}\\n   ```\\n\\n2. **히스토리**: 이는 사용자와 어시스턴트 간에 교환된 이전 메시지들로 구성됩니다. 히스토리의 각 메시지는 사용자 또는 어시스턴트로부터 온 것인지 나타내는 유사한 형식을 따릅니다.\\n\\n   ```python\\n   history  # 이는 이전 메시지들의 목록입니다.\\n   ```\\n\\n3. **사용자 메시지**: 이는 사용자의 현재 메시지입니다. 이를 통해 모델은 현재 사용자가 무엇을 요청하거나 말하는지를 이해할 수 있습니다.\\n\\n   ```python\\n   {\"role\": \"user\", \"content\": message}\\n   ```\\n\\n### 결합하기\\n\\n이 세 가지 구성 요소를 결합하면, 대화의 전체 맥락을 나타내는 완전한 입력 리스트가 형성됩니다:\\n\\n```python\\nmessages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\\n```\\n\\n- **`[{\"role\": \"system\", \"content\": system_message}]`**: 이는 시스템 메시지 하나로 구성된 리스트를 생성합니다.\\n- **`+ history`**: 이는 기존의 메시지 히스토리를 리스트에 연결합니다.\\n- **`+ [{\"role\": \"user\", \"content\": message}]`**: 이는 현재 사용자 메시지를 리스트의 마지막 항목으로 추가합니다.\\n\\n### 예시\\n\\n예를 들어, 다음과 같은 경우로 설명해 보겠습니다:\\n\\n```python\\nsystem_message = \"당신은 유용한 어시스턴트입니다.\"\\nhistory = [\\n    {\"role\": \"user\", \"content\": \"안녕하세요!\"},\\n    {\"role\": \"assistant\", \"content\": \"안녕하세요! 무엇을 도와드릴까요?\"}\\n]\\nmessage = \"파이썬에 대해 알려줄 수 있나요?\"\\n\\nmessages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\\n```\\n\\n이제 `messages` 변수는 다음과 같이 보일 것입니다:\\n\\n```python\\n[\\n    {\"role\": \"system\", \"content\": \"당신은 유용한 어시스턴트입니다.\"},\\n    {\"role\": \"user\", \"content\": \"안녕하세요!\"},\\n    {\"role\": \"assistant\", \"content\": \"안녕하세요! 무엇을 도와드릴까요?\"},\\n    {\"role\": \"user\", \"content\": \"파이썬에 대해 알려줄 수 있나요?\"}\\n]\\n```\\n\\n이 구조는 모델이 대화에 대한 모든 관련 맥락을 갖게 하여 적절한 응답을 생성할 수 있도록 합니다.\\n\\n더 궁금한 점이 있거나 추가 설명이 필요하시면 언제든지 질문해 주세요!', 'options': None}]\n",
      "And messages is:\n",
      "[{'role': 'system', 'content': 'You are a helpful assistant'}, {'role': 'user', 'metadata': None, 'content': 'Hello', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'Hello! How can I assist you today?', 'options': None}, {'role': 'user', 'metadata': None, 'content': 'It is just test about gradio', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': \"Got it! If you have any questions about Gradio or if you'd like to test something specific, feel free to ask. I'm here to help!\", 'options': None}, {'role': 'user', 'metadata': None, 'content': 'I don\\'t understand how the message of OpenAI package\\'s message can be formed as messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'In the context of using the OpenAI API, particularly for conversational models like ChatGPT, the message structure you mentioned is designed to provide context for the conversation. Let\\'s break down the components of the message array you mentioned:\\n\\n1. **System Message**: This is an initial message that sets the context or behavior of the assistant. It generally provides instructions about how the assistant should behave or what information it should consider while replying.\\n\\n   ```python\\n   {\"role\": \"system\", \"content\": system_message}\\n   ```\\n\\n2. **History**: This refers to the conversation history which usually consists of previous messages exchanged between the user and the assistant. Each message in the history follows a similar format, indicating whether the message was from the user or the assistant.\\n\\n   ```python\\n   history  # This is a list of previous messages in the same format.\\n   ```\\n\\n3. **User Message**: This is the current message from the user. It allows the model to understand what the user is asking or saying at this moment.\\n\\n   ```python\\n   {\"role\": \"user\", \"content\": message}\\n   ```\\n\\n### Combining Them\\n\\nWhen you combine these three components, the resulting list of messages will form a complete input that represents the entire conversation context for the model:\\n\\n```python\\nmessages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\\n```\\n\\n- **`[{\"role\": \"system\", \"content\": system_message}]`**: This creates a list with one item, the system message.\\n- **`+ history`**: This concatenates the existing history of messages to the list.\\n- **`+ [{\"role\": \"user\", \"content\": message}]`**: This adds the current user message as the last item in the list.\\n\\n### Example\\n\\nLet\\'s illustrate this with an example:\\n\\n```python\\nsystem_message = \"You are a helpful assistant.\"\\nhistory = [\\n    {\"role\": \"user\", \"content\": \"Hello!\"},\\n    {\"role\": \"assistant\", \"content\": \"Hi there! How can I help you today?\"}\\n]\\nmessage = \"Can you tell me about Python?\"\\n\\nmessages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\\n```\\n\\nThe `messages` variable would now look like this:\\n\\n```python\\n[\\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\\n    {\"role\": \"user\", \"content\": \"Hello!\"},\\n    {\"role\": \"assistant\", \"content\": \"Hi there! How can I help you today?\"},\\n    {\"role\": \"user\", \"content\": \"Can you tell me about Python?\"}\\n]\\n```\\n\\nThis structure allows the model to have all relevant context about the conversation so it can generate an appropriate response.\\n\\nIf you have more questions or need clarification, feel free to ask!', 'options': None}, {'role': 'user', 'metadata': None, 'content': 'Can you translate it to Korean?', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': '물론입니다! 위에서 설명한 내용을 한국어로 번역해 드리겠습니다.\\n\\n---\\n\\nOpenAI API를 사용할 때, 특히 ChatGPT와 같은 대화형 모델에 대해 말씀하신 메시지 구조는 대화의 맥락을 제공하기 위해 설계되었습니다. 구성 요소를 나눠서 살펴보겠습니다:\\n\\n1. **시스템 메시지**: 이는 어시스턴트의 맥락이나 행동을 설정하는 초기 메시지입니다. 일반적으로 어시스턴트가 어떻게 행동해야 하는지 또는 어떤 정보를 고려해야 하는지에 대한 지침을 제공합니다.\\n\\n   ```python\\n   {\"role\": \"system\", \"content\": system_message}\\n   ```\\n\\n2. **히스토리**: 이는 사용자와 어시스턴트 간에 교환된 이전 메시지들로 구성됩니다. 히스토리의 각 메시지는 사용자 또는 어시스턴트로부터 온 것인지 나타내는 유사한 형식을 따릅니다.\\n\\n   ```python\\n   history  # 이는 이전 메시지들의 목록입니다.\\n   ```\\n\\n3. **사용자 메시지**: 이는 사용자의 현재 메시지입니다. 이를 통해 모델은 현재 사용자가 무엇을 요청하거나 말하는지를 이해할 수 있습니다.\\n\\n   ```python\\n   {\"role\": \"user\", \"content\": message}\\n   ```\\n\\n### 결합하기\\n\\n이 세 가지 구성 요소를 결합하면, 대화의 전체 맥락을 나타내는 완전한 입력 리스트가 형성됩니다:\\n\\n```python\\nmessages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\\n```\\n\\n- **`[{\"role\": \"system\", \"content\": system_message}]`**: 이는 시스템 메시지 하나로 구성된 리스트를 생성합니다.\\n- **`+ history`**: 이는 기존의 메시지 히스토리를 리스트에 연결합니다.\\n- **`+ [{\"role\": \"user\", \"content\": message}]`**: 이는 현재 사용자 메시지를 리스트의 마지막 항목으로 추가합니다.\\n\\n### 예시\\n\\n예를 들어, 다음과 같은 경우로 설명해 보겠습니다:\\n\\n```python\\nsystem_message = \"당신은 유용한 어시스턴트입니다.\"\\nhistory = [\\n    {\"role\": \"user\", \"content\": \"안녕하세요!\"},\\n    {\"role\": \"assistant\", \"content\": \"안녕하세요! 무엇을 도와드릴까요?\"}\\n]\\nmessage = \"파이썬에 대해 알려줄 수 있나요?\"\\n\\nmessages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\\n```\\n\\n이제 `messages` 변수는 다음과 같이 보일 것입니다:\\n\\n```python\\n[\\n    {\"role\": \"system\", \"content\": \"당신은 유용한 어시스턴트입니다.\"},\\n    {\"role\": \"user\", \"content\": \"안녕하세요!\"},\\n    {\"role\": \"assistant\", \"content\": \"안녕하세요! 무엇을 도와드릴까요?\"},\\n    {\"role\": \"user\", \"content\": \"파이썬에 대해 알려줄 수 있나요?\"}\\n]\\n```\\n\\n이 구조는 모델이 대화에 대한 모든 관련 맥락을 갖게 하여 적절한 응답을 생성할 수 있도록 합니다.\\n\\n더 궁금한 점이 있거나 추가 설명이 필요하시면 언제든지 질문해 주세요!', 'options': None}, {'role': 'user', 'content': 'But Gradio has been upgraded! Now it will pass in history in the exact OpenAI format, perfect for us to send straight to OpenAI.\\n\\n그럼 이건 무슨뜻일까'}]\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f91b414-8bab-472d-b9c9-3fa51259bdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant in a clothes store. You should try to gently encourage \\\n",
    "the customer to try items that are on sale. Hats are 60% off, and most other items are 50% off. \\\n",
    "For example, if the customer says 'I'm looking to buy a hat', \\\n",
    "you could reply something like, 'Wonderful - we have lots of hats - including several that are part of our sales event.'\\\n",
    "Encourage the customer to buy hats if they are unsure what to get.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e5be3ec-c26c-42bc-ac16-c39d369883f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "413e9e4e-7836-43ac-a0c3-e1ab5ed6b136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7872\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7872/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d75f0ffa-55c8-4152-b451-945021676837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another Multi-shot Prompting\n",
    "\n",
    "system_message += \"\\nIf the customer asks for shoes, you should respond that shoes are not on sale today, \\\n",
    "but remind the customer to look at hats!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c602a8dd-2df7-4eb7-b539-4e01865a6351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7874\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7874/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a987a66-1061-46d6-a83a-a30859dc88bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed a bug in this function brilliantly identified by student Gabor M.!\n",
    "# I've also improved the structure of this function\n",
    "def chat(message, history):\n",
    "\n",
    "    relevant_system_message = system_message\n",
    "    # 현재 User Prompt가 'belt'라는 단어를 포함하면 system_message에 추가\n",
    "    # 이것은 그냥 예시이고, 특정 단어를 찾으려면 보통 사전 등을 이용함 (RAG)\n",
    "    if 'belt' in message:\n",
    "        relevant_system_message += \" The store does not sell belts; if you are asked for belts, be sure to point out other items on sale.\"\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": relevant_system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20570de2-eaad-42cc-a92c-c779d71b48b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7875\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7875/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a57ee0-b945-48a7-a024-01b56a5d4b3e",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business Applications</h2>\n",
    "            <span style=\"color:#181;\">Conversational Assistants are of course a hugely common use case for Gen AI, and the latest frontier models are remarkably good at nuanced conversation. And Gradio makes it easy to have a user interface. Another crucial skill we covered is how to use prompting to provide context, information and examples.\n",
    "<br/><br/>\n",
    "Consider how you could apply an AI Assistant to your business, and make yourself a prototype. Use the system prompt to give context on your business, and set the tone for the LLM.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfb9e21-df67-4c2b-b952-5e7e7961b03d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
