# Regularization

>모델이 Overfitting을 피하여 일반화 성능을 잃지 않도록 가중치를 제한하는 방법으로, 과대 적합을 완화하기 위한 대표적인 방법

# Norm
>어떤 벡터의 크기(길이)나 벡터간의 거리를 측정하는 방법 

**Vector's norm** ![](https://velog.velcdn.com/images/kvvon/post/1d21fd36-514a-4798-9aa6-e2fe3392dcdd/image.png)
- 이 방법은 **p-norm** 이라고 불린다.

**Matrix's norm** ![](https://velog.velcdn.com/images/kvvon/post/070e87b0-90c4-4548-8395-3482c4e18a6c/image.png)

### Manhattan norm (L1 norm)
> **p = 1인 P-Norm**

- 각 element의 절댓값의 합과 동일하다.
- 각 element의 값의 변화를 확인하는데 유용하다.

### Euclidean norm (L2 norm)
> **p = 2인 P-Norm**

- 원점과 해당 벡터의 거리와 동일하다.
- n차원 좌표평면(유클리드 공간)에서의 벡터의 크기를 계산한다.

#### Manhattan vs Euclidean
- **맨허튼 거리는 유클리드 거리보다 항상 같거나 크다.**
- 유클리드 거리는 최소 직선 거리를 구하는 반면, 맨허튼 거리는 $x, y$ 좌표를 통해 갈 수 있는 최소의 거리를 구한다.
### Maximum norm
![](https://velog.velcdn.com/images/kvvon/post/3d53c19e-0bd6-45d4-9a65-4430fc2f416a/image.png)

# Regularization
![](https://velog.velcdn.com/images/kvvon/post/3624ff13-f322-4b92-8fb1-940c5da9e4d7/image.png) 
> Regularization을 제약 조건이 있는 최적화로 생각할 수 있다. 즉, λ는 라그랑주 상수이고 제약 조건을 C보다 작게 유지해야한다.

- 그림 상에서 파란색 부분이 제약조건이며 해당 조건 아래에서 L이 최소가 되는 파라미터 (가중치)를 찾아야한다. 
- 사이킷런의 LogisticRegression(C=100.0, solver='lbfgs', multi_class='ovr') 에서 **'C='** 에 해당하는 부분이 **최대 복잡도 허용도** 이다.
	
    - 일반적으로 **C = 1 / $\lambda$** 이다.
    ![](https://velog.velcdn.com/images/kvvon/post/4a7be793-42bd-4c94-9f98-d608d930ba1c/image.png)

## L1 Regularization (Lasso Regression)
Penalty term으로 L1 norm을 사용하는 Regularization
![](https://velog.velcdn.com/images/kvvon/post/f5925c52-f744-40b8-97a0-ab79075574b8/image.png)

- 제약 조건이 절댓값 이기 때문에 마름모 꼴로 나타난다.
- 최적값은 모서리 부분에서 나타날 확률이 릿지에 비해 높기 때문에 몇몇 유의미하지 않은 변수들에 대해 계수를 0에 가깝게 (또는 0) 으로 추정해 **feature selection의 효과를** 가져온다.
![](https://velog.velcdn.com/images/kvvon/post/40ae856f-6f25-4274-8fba-c61886652b28/image.png)

## L2 Regularization (Ridge regression)
Penalty term으로 L2 norm을 사용하는 Regularization
![](https://velog.velcdn.com/images/kvvon/post/2864230a-8f6a-4790-b786-b0beff60ec54/image.png)

- 모든 변수의 가중치를 0에 가깝게 줄이지만, 정확히 0으로 만들지는 않는다.
- 모든 변수를 고려해야될 때 사용한다.
![](https://velog.velcdn.com/images/kvvon/post/3e4ae331-734b-4712-8377-4a6f6cf93c16/image.png)
- 만약 MSE = 0 이라고 하더라도 0으로 만들지 않고 0과 비슷한 작은 값으로 초기화한다.

### Lasso vs Ridge
![](https://velog.velcdn.com/images/kvvon/post/b033b79f-bbb8-4e57-8cb0-b87d6d59ea21/image.png)
- **다중공선성:** 회귀 분석에서 독립 변수들(특성들) 사이의 상관관계가 매우 높을 때 발생하는 문제
	
    - Overfitting의 가능성이 높아진다.
### 사이킷런으로 Lasso regression 구현
```python
from sklearn.linear_model import Lasso
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_diabetes
import numpy as np

# 데이터 로드
X, y = load_diabetes(return_X_y=True)

# 학습 데이터 분리
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Lasso 모델 생성 및 학습 (α가 클수록 규제가 강함)
lasso = Lasso(alpha=0.1)  # alpha = λ (규제 강도)
lasso.fit(X_train, y_train)

# 결과 확인
print("선택된 특성 개수:", np.sum(lasso.coef_ != 0))
print("Lasso 회귀 계수:", lasso.coef_)
print("모델 성능 (R^2):", lasso.score(X_test, y_test))
```