# Perceptron with sklearn
```python
from sklearn import datasets #데이터셋
from sklearn.model_selection import train_test_split #데이터셋 분리
from sklearn.preprocessing import StandardScaler #스케일 조정
from sklearn.linear_model import Perceptron
from sklearn.metrics import accuracy_score
import numpy as np

```

sklearn에 내장되어있는 데이터셋을 사용해보자. 

우리는 2가지 특성만을 사용할 것이다.
- X = iris.data[:, [2,3]]

내장되어있는 데이터에는 각 label이 정수로 라벨링되어있다.
- 작은 메모리 영역만을 차지하므로 계산 성능이 향상되기 때문이다.

**np.unique(y) 함수**는 y에 미리 라벨링 되어있는 정수들을 배열로 반환한다.

```python
iris = datasets.load_iris()
X = iris.data[:, [2,3]] #3번째, 4번째 특성만 사용
y = iris.target #정수로 레이블 되어져있다.
#unique(y) >> y에 저장된 클래스 레이블 반환환
print('클래스 레이블:', np.unique(y)) #클래스 레이블: [0 1 2]
```

우리에게 주어진 데이터셋은 하나뿐이기 때문에 훈련 데이터와 테스트 데이터로 나누어야한다.

**train_test_split() 함수**를 통해 전체 데이터셋을 test__data 30%, training_data 70%로 나누어보자.

이때 **stratify=y**를 통해 계층화를 해주어야한다. 
- 훈련 데이터셋, 테스트 데이터셋 내에서 각 클래스 레이블의 비율을 동일하게 만들어주기 위함이다.

**np.bincount() 함수**를 통해 출력해보면 모든 클래스 레이블의 개수가 동일함을 확인할 수 있다. 
```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)
#test_size = 0.4 >> 전체 데이터 중 30%를 테스트 데이터, 70%를 훈련 데이터로 나눔
#random_state = 1 >> 데이터를 split 이전에 섞어야 각 데이터에 여러 레이블이 포함됨
#stratify=y: stratification 기능 >> 클래스 레이블 간의 개수가 일치하도록 0:15 == 1:15 == 2:15

#bincount(y) >> vector y[i]에 맞는 element에 개수를 세어 vector로 반환
print('y의 레이블 카운트:', np.bincount(y)) #y의 레이블 카운트: [50 50 50]
print('y_train의 레이블 카운트:', np.bincount(y_train)) #y_train의 레이블 카운트: [35 35 35]
print('y_test의 레이블 카운트:', np.bincount(y_test)) #y_test의 레이블 카운트: [15 15 15]
```

본격적으로 학습시키기에 앞서, 데이터들의 크기를 조정해보자.

**sc를 StandardScaler() 클래스의 객체**로 사용함으로써 크기를 조정할 수 있다.

크기를 조정하는 이유는 다음과 같다.
- 가중치를 조정하는 식은 다음과 같다.
- $$ΔW_i = η(y_i − \hat{y_i}) \cdot X_i$$
- 만약 특정 데이터의 크기만 과하게 크다면 해당 데이터에 대한 가중치가 무의미하게 커질 수 있다.
- 이를 방지하기 위해 정규화를 진행한다.
	
    - 정규화: $\frac{x-u}{std}$
    
**fit(X_train)** 을 통해 training data에 맞는 평균과 표준편차를 계산하고 이를 이용하여 데이터를 정규화한다.
```python
sc = StandardScaler()
sc.fit(X_train) #훈련 데이터 셋의 특성(열)마다 평균과 표준편차 계산
#transform(): 위와 동일한 평균과 표준편차를 이용해 데이터셋을 표준화화
X_train_std = sc.transform(X_train) 
X_test_std = sc.transform(X_test)
```
이제 **Perceptron() 클래스**를 통해 학습을 진행할 수 있다.

**Perceptron.fit() 함수**를 통해 Training data에 대해 학습을 진행한다. 

```python
ppn = Perceptron(eta=0.1, random_state=1)
#여기서 random_state는 epoch마다 훈련 데이터셋을 섞는 역할
ppn.fit(X_train_std, y_train)
```

잘못 분류된 데이터의 개수와 정확도를 출력해볼 수 있다.

**Perceptron.predict() 함수**는 training data에 대한 학습값을 배열로 반환한다.

**accuracy_score() 와 Perceptron.score() 함수**는 정확도를 계산해준다.
```python
y_pred = ppn.predict(X_test_std)
print('잘못 분류된 샘플 개수: %d' % (y_test != y_pred).sum())

print('정확도 %.3f' %accuracy_score(y_test, y_pred)) #정확도: 0.978
print('정확도: %.3f' %ppn.score(X_test_std, y_test)) #정확도: 0.978
```

퍼셉트론은 간단하지만 선형 분류만 가능하다.

따라서 선형 분류가 불가능한 데이터셋에 대해서는 절대 수렴할 수 없다.