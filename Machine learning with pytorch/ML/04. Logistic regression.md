# Logistic Regression
> 퍼셉트론보다 강력한 이진 분류 모델

**odds**: 특정 사건이 발생할 확률과 발생하지 않을 확률의 비율
- $\frac{p}{(1-p)}$
- 여기서 p는 $p(y = 1 | x)$ 로 정의할 수 있다.

$logit(P) = log\frac{p}{(1-p)}$ 로 정의해보자.
- $logit(P) = W^tX + b$ 로 가정할 수 있다. 
- 로지스틱 모델에서는 가중치가 적용된 입력과 Log odds 사이에 **선형 관계가 있다고 가정한다.**
- $logit(P)$ 의 역할은 확률 P를 실수로 매핑하는 것이다.
- **사실 우리가 해야하는 일은 실수값을 확률 P로 매핑해야한다. **
	
    - 이를 위해 Logit의 역함수를 이용할 수 있다.
    
### Logistic sigmoid function   
아래의 과정을 거쳐 실수 범위를 확률 P로 매핑하게 도와주는 함수
![](https://velog.velcdn.com/images/kvvon/post/d75f348c-3524-46d0-a3fe-67f5cfc41601/image.png)시그모이드 함수의 모양은 다음과 같다.
![](https://velog.velcdn.com/images/kvvon/post/8d4f43aa-cdd8-44ad-ad58-0f73c4d5aa99/image.png)

앞서 시그모이드 함수의 값은 조건부확률 P에 매핑된다고 했다.
**즉, 시그모이드 함수의 값은 데이터와 파라미터가 주어졌을 때, Label 1에 해당될 확률을 나타낸다.**

결과값 $\hat{y}$: $\sigma(z) \ge 0.5$ 라면 클래스 1로 분류하고 ($\hat{y} = 1$) , $\hat{y}$: $\sigma(z) < 0.5$ 라면 클래스 0으로 분류한다. ($\hat{y} = 0$)

로지스틱 회귀에서 우리는 **입력 Z 대신, $W^tX + b$ 를 사용한다**. 

# Logistic Regression
![](https://velog.velcdn.com/images/kvvon/post/4d6b89e3-d8e7-4ec1-904e-9fa1d789cd6d/image.png)

아달린과 마찬가지로, 로지스틱 회귀에서도 최대화 또는 최소화하려는 함수가 필요하다. 
로지스틱 회귀에서는 Likelihood를 사용할 것이며 우리는 이를 최대화하는 파라미터를 찾아야한다.
- **likelihood**: $L(x, \theta)$ 에서 x가 주어진 경우에, $\theta$ 가 얼마나 적합한지 확인한다.
- $L(x, \theta) = P(x, \theta)$
 ![](https://velog.velcdn.com/images/kvvon/post/3333900f-d581-4bf1-abec-6ba6e29bc16f/image.png)

likelihood를 먼저 유도해보자.
- X가 주어졌을 때, X가 class 1에 속할 확률, class 0에 속할 확률은 맨 위와 동일하다. 
- 이것을 이용하여 베르누이 변수의 확률 질량 함수를 얻으면 위와 같이 나온다. ![](https://velog.velcdn.com/images/kvvon/post/d33a0eb4-157d-44f2-a64a-e7cc0da475fa/image.png)

먼저 데이터셋에 존재하는 각 샘플이 모두 독립적이라고 가정하자.
- 그럼 Likelihood는 모든 확률(시그모이드 함수)을 곱합 것과 같다.
- Likelihood에 로그를 취하면 모든 곱이 합 (시그마)로 바뀐다.
- log를 취했기 때문에 지수를 앞으로 내리면 최종식이 완성된다.

위 식은 **Log likelihood function** 이라고 한다.
- 이 식을 경사 상승법과 같은 최적화 알고리즘을 통해 최대화할 수 있다.
- 손실 함수 L로 표현하여 최소화할 수도 있다.
- 보통 값이 클수록 데이터에 적합한 파라미터임을 의미한다.

### 수학적 증명
![](https://velog.velcdn.com/images/kvvon/post/ea43cd23-e553-4591-847d-53dc0b82e67d/image.png) 위 과정을 거쳐 $w_i, b$ 가 업데이트 된다.

### Log likelihood function 이나 Likelihood function이 클수록 좋은 이유
앞선 베르누이 확률 분포에서 지수에 위치하는 $y_i =$ {0, 1}만 가지는 이산 확률 변수이다.
**$y_i$ == 예측값 (시그모이드 함수값)**
- 베르누이 분포에 따른 확률 질량 함수의 값은 시그모이드 함수의 값이다.
- 이때 시그모이드 함수의 값은 해당 데이터가 클래스 $y_i$에 속할 확률이다.
- 이 경우, 예측값이 실제값과 같으므로 시그모이드의 함수의 값은 비교적 클 것이다.

**$y_i$ != 예측값 (시그모이드 함수값)**
- 베르누이 분포에 따른 확률 질량 함수의 값은 시그모이드 함수의 값이다.
- 이때 시그모이드 함수의 값은 해당 데이터가 클래스 $y_i$에 속하지 않을 확률이다.
- 이 경우, 예측값이 실제값과 다르므로 시그모이드의 함수의 값은 비교적 작을 것이다.

> **따라서 각 시그모이드 함수의 값을 모두 더하거나 곱한 Likelihood가 클수록 데이터에 적합한 파라미터를 선택하였다고 할 수 있다.**

아래 코드에서는 Log likelihood function을 손실 함수로 이용하고, 손실 함수를 최소화하기 위해 - Log likelihood function에 -를 붙여 사용할 것이다.
- '-'를 붙였을 때, 최소화하면 최적인 이유는 (1-$\sigma$(z)) 와 $\sigma$(z) 가 1보다 작은 수라는 점과 로그 함수의 개형을 생각해보면 알 수 있다.

### 시그모이드 함수를 사용한다면 얻을 수 있는 장점
![](https://velog.velcdn.com/images/kvvon/post/894bf13b-a887-44eb-b7e7-53a51ed94b0d/image.png)


## Loss function이 Log likelihood function인 아달린 구현

```python
import numpy as np

class logisticRegressionGD:
    def __init__(self, eta=0.01, n_iter=50, random_state=1):      
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state
    
    def fit(self, X, y): #X: matrix, y: vector
        rgen = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc = 0.0, scale=0.01, size=X.shape[1])
        self.b_ = np.float64(0.)
        self.losses_ = []

        for _ in range(self.n_iter):
            net_input = self.net_input(X)
            output = self.activation(net_input)
            errors = (y-output)
            self.w_ += self.eta * 2.0 * X.T.dot(errors) / X.shape[0]
            self.b_ += self.eta * 2.0 * errors.mean() 
            loss = (-y.dot(np.log(output)) - ((1-y).dot(np.log(1-output)))/X.shape[0])
            #데이터별 평균 loss를 나타냄. 데이터 수인 X.shape[0] 으로 나누는 것은 크게 상관없다.
            self.losses_.append(loss)
    
    def net_input(self, X):
        return np.dot(X, self.w_) + self.b_ 

    def activation(self, z): 
        #numpy.clip(array, min, max): array 내의 element들에 대해서 min보다 작은 값들을 min으로 바꿔주고 
        #max보다 큰 값들을 max값으로 바꿔주는 함수.
        return 1. / (1. + np.exp(-np.clip(z, -250, 250)))
        #250, -250은 exp의 범위 오류를 제한하기 위한 값이다.

    def predict(self, X):
        return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)
```

## 사이킷런으로 Logistic regression model 구현
```python
from sklearn.linear_model import LogisticRegression
#C: 모델을 데이터에 맞출 정도 (C가 작을수록 정규화가 강하다)
#solver: 로지스틱 모델을 훈련할 알고리즘 종류
#L-BFGS (Limited-memory Broyden–Fletcher–Goldfarb–Shanno) 알고리즘
	#multi_class: 다중 클래스 분류 방법 선택
lr = LogisticRegression(C=100.0, solver='lbfgs', multi_class='ovr')
lr.fit(X_train_std, y_train)
#matplotlib에서 영역 나누기 
#분류기준: classifier = lr
plot_decision_regions(X_combined_std, y_combine, classifier=lr, test_idx=range(105,150))
```

사이킷런의 LogisticRegression은 이진 분류뿐만이 아니라 다중 분류도 지원한다.
- **OVR vs MULTINOMIAL**![](https://velog.velcdn.com/images/kvvon/post/28c45ed4-5dd4-42a6-93dd-07fbfe597803/image.png)

```python
# 각 훈련 샘플이 각 클래스마다 속할 확률을 배열로 반환
lr.predict_proba(X_test_std[:3, :])
# 각 훈련 샘플이 속할 확률이 가장 큰 클래스 레이블만을 반환
lr.predict_proba(X_test_std[:3, :]).argmax(axis=1)
```

## Regularization을 이용하여 Overfitting 예방하기
머신러닝으로 분류기를 만들 때 발생하는 에러들을 MSE로 분석해보면 아래의 식이 나온다. ![](https://velog.velcdn.com/images/kvvon/post/101ad9d3-1436-4157-9c62-6b7b5dee7b56/image.png) ![](https://velog.velcdn.com/images/kvvon/post/1a294859-7b0e-4781-9cdc-b6a06a8fa681/image.png) 
- **Bias**: 데이터의 모든 정보를 고려하지 않음으로 인해 데이터를 제대로 분류하지 못하는 경우
	
    - Test data에 대한 평균 에러 (MSE)
    - **Underfitting**에 비례한다.
- **Variance**: 학습 파라미터가 훈련 데이터셋만 정확히 분류하고 테스트 데이터는 분류하지 못하는 경우
	
    - Test data에 대한 예측의 분산값
    - **Overfitting**에 비례한다.
- **Irreducible error**: 데이터가 가지는 한계치
![](https://velog.velcdn.com/images/kvvon/post/ca0a6bd1-e764-4bf7-9c55-ff8cf5bee822/image.png)
> 우리의 목표는 **Bias와 Variance 모두를 줄이는 것이다.**

**Regularization**: Overfitting을 줄이는 방법 중 하나
- 과도한 파라미터의 값을 제한하기 위해 추가적인 정보를 주입하는 개녑
- Logistic regression에 대한 Regulation에서는 **L2 Regularization**을 사용할 것이다.

**L2 Regularization (Ridge regression)**![](https://velog.velcdn.com/images/kvvon/post/a9725f16-488d-460b-9933-12adde357975/image.png)
- $\Delta w$에 해당하는 부분이 커지도록 하여 $w$가 Overfitting 되지 않도록 돕는다.
- 릿지 회귀는 경사 하강법을 목적 함수에 적용할 때 **값이 큰 가중치는 빠르게 작아지고, 값이 작은 가중치는 비교적 천천히 작아지게 한다.**

```python
from sklearn.linear_model import LogisticRegression

# L2 규제 사용 (penalty='l2'는 기본값, l1은 L1 Regularization을 의미)
model = LogisticRegression(penalty='l2', C=1.0, solver='lbfgs')
```

**C가 작을수록 정규화가 강한 이유**
- 사이킷런의 LogisticRegression은 기본적으로 L2 규제를 사용한다.
- **L2 규제에서 $C = \frac{1}{\lambda}$ 으로 사용되기 때문에, C가 작을수록 L2 규제가 강하다.**

